{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense, LSTM, Dropout\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.models import Sequential\n",
    "import autokeras as ak\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import reber\n",
    "RANDOM_STATE = 42\n",
    "PADDING_IDX = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = reber.ReberGenerator(max_length=25)\n",
    "X, y = r.make_data(num_rows=100000)\n",
    "_, word_len = X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1, random_state=RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_vector_length = len(r._reber_letters) # TODO: what should this be?\n",
    "num_neurons = 60\n",
    "model = Sequential()\n",
    "model.add(\n",
    "    Embedding(\n",
    "        embedding_vector_length + 1,\n",
    "        embedding_vector_length,\n",
    "        input_length=word_len,\n",
    "        mask_zero=True\n",
    "    )\n",
    ")\n",
    "model.add(LSTM(num_neurons))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reloading Oracle from existing project ./auto_model/oracle.json\n",
      "Train for 2000 steps, validate for 500 steps\n",
      "Epoch 1/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 339/2000 [====>.........................] - ETA: 1:00:10 - loss: 0.6952 - accuracy: 0.437 - ETA: 20:45 - loss: 0.6941 - accuracy: 0.4792  - ETA: 12:52 - loss: 0.6918 - accuracy: 0.500 - ETA: 9:29 - loss: 0.6919 - accuracy: 0.504 - ETA: 7:36 - loss: 0.6927 - accuracy: 0.49 - ETA: 6:24 - loss: 0.6935 - accuracy: 0.49 - ETA: 5:35 - loss: 0.6924 - accuracy: 0.50 - ETA: 5:21 - loss: 0.6920 - accuracy: 0.51 - ETA: 4:51 - loss: 0.6916 - accuracy: 0.51 - ETA: 4:26 - loss: 0.6918 - accuracy: 0.51 - ETA: 4:06 - loss: 0.6919 - accuracy: 0.51 - ETA: 3:49 - loss: 0.6909 - accuracy: 0.52 - ETA: 3:36 - loss: 0.6888 - accuracy: 0.52 - ETA: 3:24 - loss: 0.6887 - accuracy: 0.52 - ETA: 3:15 - loss: 0.6897 - accuracy: 0.52 - ETA: 3:12 - loss: 0.6892 - accuracy: 0.52 - ETA: 3:10 - loss: 0.6892 - accuracy: 0.51 - ETA: 3:07 - loss: 0.6883 - accuracy: 0.52 - ETA: 3:05 - loss: 0.6873 - accuracy: 0.52 - ETA: 3:03 - loss: 0.6880 - accuracy: 0.52 - ETA: 3:01 - loss: 0.6875 - accuracy: 0.52 - ETA: 2:59 - loss: 0.6861 - accuracy: 0.52 - ETA: 2:53 - loss: 0.6859 - accuracy: 0.53 - ETA: 2:49 - loss: 0.6836 - accuracy: 0.53 - ETA: 2:48 - loss: 0.6823 - accuracy: 0.54 - ETA: 2:43 - loss: 0.6823 - accuracy: 0.54 - ETA: 2:38 - loss: 0.6826 - accuracy: 0.54 - ETA: 2:34 - loss: 0.6811 - accuracy: 0.54 - ETA: 2:30 - loss: 0.6792 - accuracy: 0.54 - ETA: 2:27 - loss: 0.6774 - accuracy: 0.55 - ETA: 2:23 - loss: 0.6772 - accuracy: 0.55 - ETA: 2:20 - loss: 0.6767 - accuracy: 0.55 - ETA: 2:17 - loss: 0.6758 - accuracy: 0.55 - ETA: 2:15 - loss: 0.6744 - accuracy: 0.55 - ETA: 2:12 - loss: 0.6742 - accuracy: 0.55 - ETA: 2:10 - loss: 0.6727 - accuracy: 0.56 - ETA: 2:07 - loss: 0.6718 - accuracy: 0.56 - ETA: 2:05 - loss: 0.6723 - accuracy: 0.56 - ETA: 2:03 - loss: 0.6702 - accuracy: 0.56 - ETA: 2:01 - loss: 0.6709 - accuracy: 0.56 - ETA: 2:00 - loss: 0.6697 - accuracy: 0.56 - ETA: 1:58 - loss: 0.6701 - accuracy: 0.56 - ETA: 1:58 - loss: 0.6697 - accuracy: 0.56 - ETA: 1:57 - loss: 0.6694 - accuracy: 0.56 - ETA: 1:55 - loss: 0.6691 - accuracy: 0.56 - ETA: 1:54 - loss: 0.6667 - accuracy: 0.57 - ETA: 1:52 - loss: 0.6665 - accuracy: 0.57 - ETA: 1:51 - loss: 0.6652 - accuracy: 0.57 - ETA: 1:50 - loss: 0.6632 - accuracy: 0.57 - ETA: 1:49 - loss: 0.6631 - accuracy: 0.57 - ETA: 1:49 - loss: 0.6613 - accuracy: 0.57 - ETA: 1:49 - loss: 0.6605 - accuracy: 0.58 - ETA: 1:48 - loss: 0.6583 - accuracy: 0.58 - ETA: 1:47 - loss: 0.6586 - accuracy: 0.58 - ETA: 1:46 - loss: 0.6592 - accuracy: 0.58 - ETA: 1:45 - loss: 0.6579 - accuracy: 0.58 - ETA: 1:44 - loss: 0.6562 - accuracy: 0.58 - ETA: 1:43 - loss: 0.6548 - accuracy: 0.58 - ETA: 1:42 - loss: 0.6544 - accuracy: 0.58 - ETA: 1:41 - loss: 0.6541 - accuracy: 0.58 - ETA: 1:40 - loss: 0.6529 - accuracy: 0.58 - ETA: 1:39 - loss: 0.6519 - accuracy: 0.58 - ETA: 1:39 - loss: 0.6517 - accuracy: 0.58 - ETA: 1:38 - loss: 0.6507 - accuracy: 0.58 - ETA: 1:37 - loss: 0.6493 - accuracy: 0.59 - ETA: 1:36 - loss: 0.6494 - accuracy: 0.59 - ETA: 1:36 - loss: 0.6489 - accuracy: 0.59 - ETA: 1:36 - loss: 0.6490 - accuracy: 0.59 - ETA: 1:35 - loss: 0.6481 - accuracy: 0.59 - ETA: 1:35 - loss: 0.6465 - accuracy: 0.59 - ETA: 1:34 - loss: 0.6457 - accuracy: 0.59 - ETA: 1:34 - loss: 0.6460 - accuracy: 0.59 - ETA: 1:34 - loss: 0.6457 - accuracy: 0.59 - ETA: 1:33 - loss: 0.6434 - accuracy: 0.60 - ETA: 1:33 - loss: 0.6425 - accuracy: 0.60 - ETA: 1:32 - loss: 0.6408 - accuracy: 0.60 - ETA: 1:32 - loss: 0.6392 - accuracy: 0.60 - ETA: 1:31 - loss: 0.6389 - accuracy: 0.60 - ETA: 1:30 - loss: 0.6376 - accuracy: 0.60 - ETA: 1:30 - loss: 0.6371 - accuracy: 0.60 - ETA: 1:29 - loss: 0.6364 - accuracy: 0.60 - ETA: 1:29 - loss: 0.6359 - accuracy: 0.60 - ETA: 1:28 - loss: 0.6356 - accuracy: 0.60 - ETA: 1:28 - loss: 0.6348 - accuracy: 0.61 - ETA: 1:27 - loss: 0.6345 - accuracy: 0.61 - ETA: 1:27 - loss: 0.6332 - accuracy: 0.61 - ETA: 1:27 - loss: 0.6322 - accuracy: 0.61 - ETA: 1:27 - loss: 0.6309 - accuracy: 0.61 - ETA: 1:27 - loss: 0.6306 - accuracy: 0.61 - ETA: 1:27 - loss: 0.6299 - accuracy: 0.61 - ETA: 1:26 - loss: 0.6295 - accuracy: 0.61 - ETA: 1:26 - loss: 0.6282 - accuracy: 0.61 - ETA: 1:26 - loss: 0.6267 - accuracy: 0.61 - ETA: 1:25 - loss: 0.6254 - accuracy: 0.62 - ETA: 1:25 - loss: 0.6256 - accuracy: 0.62 - ETA: 1:24 - loss: 0.6252 - accuracy: 0.62 - ETA: 1:24 - loss: 0.6241 - accuracy: 0.62 - ETA: 1:24 - loss: 0.6237 - accuracy: 0.62 - ETA: 1:23 - loss: 0.6228 - accuracy: 0.62 - ETA: 1:23 - loss: 0.6224 - accuracy: 0.62 - ETA: 1:23 - loss: 0.6215 - accuracy: 0.62 - ETA: 1:22 - loss: 0.6204 - accuracy: 0.62 - ETA: 1:22 - loss: 0.6198 - accuracy: 0.62 - ETA: 1:22 - loss: 0.6193 - accuracy: 0.62 - ETA: 1:21 - loss: 0.6172 - accuracy: 0.62 - ETA: 1:21 - loss: 0.6166 - accuracy: 0.62 - ETA: 1:21 - loss: 0.6170 - accuracy: 0.62 - ETA: 1:20 - loss: 0.6156 - accuracy: 0.62 - ETA: 1:20 - loss: 0.6147 - accuracy: 0.62 - ETA: 1:20 - loss: 0.6145 - accuracy: 0.62 - ETA: 1:20 - loss: 0.6141 - accuracy: 0.63 - ETA: 1:19 - loss: 0.6130 - accuracy: 0.63 - ETA: 1:19 - loss: 0.6116 - accuracy: 0.63 - ETA: 1:19 - loss: 0.6105 - accuracy: 0.63 - ETA: 1:19 - loss: 0.6097 - accuracy: 0.63 - ETA: 1:18 - loss: 0.6090 - accuracy: 0.63 - ETA: 1:18 - loss: 0.6085 - accuracy: 0.63 - ETA: 1:18 - loss: 0.6079 - accuracy: 0.63 - ETA: 1:18 - loss: 0.6074 - accuracy: 0.63 - ETA: 1:18 - loss: 0.6066 - accuracy: 0.63 - ETA: 1:18 - loss: 0.6066 - accuracy: 0.63 - ETA: 1:17 - loss: 0.6053 - accuracy: 0.64 - ETA: 1:17 - loss: 0.6048 - accuracy: 0.64 - ETA: 1:17 - loss: 0.6042 - accuracy: 0.64 - ETA: 1:17 - loss: 0.6035 - accuracy: 0.64 - ETA: 1:17 - loss: 0.6031 - accuracy: 0.64 - ETA: 1:17 - loss: 0.6026 - accuracy: 0.64 - ETA: 1:17 - loss: 0.6017 - accuracy: 0.64 - ETA: 1:17 - loss: 0.6001 - accuracy: 0.64 - ETA: 1:17 - loss: 0.5998 - accuracy: 0.64 - ETA: 1:16 - loss: 0.5985 - accuracy: 0.64 - ETA: 1:17 - loss: 0.5981 - accuracy: 0.64 - ETA: 1:17 - loss: 0.5977 - accuracy: 0.64 - ETA: 1:16 - loss: 0.5969 - accuracy: 0.64 - ETA: 1:16 - loss: 0.5961 - accuracy: 0.64 - ETA: 1:16 - loss: 0.5954 - accuracy: 0.65 - ETA: 1:16 - loss: 0.5941 - accuracy: 0.65 - ETA: 1:15 - loss: 0.5936 - accuracy: 0.65 - ETA: 1:15 - loss: 0.5925 - accuracy: 0.65 - ETA: 1:15 - loss: 0.5912 - accuracy: 0.65 - ETA: 1:15 - loss: 0.5900 - accuracy: 0.65 - ETA: 1:14 - loss: 0.5901 - accuracy: 0.65 - ETA: 1:14 - loss: 0.5901 - accuracy: 0.65 - ETA: 1:14 - loss: 0.5898 - accuracy: 0.65 - ETA: 1:14 - loss: 0.5893 - accuracy: 0.65 - ETA: 1:14 - loss: 0.5890 - accuracy: 0.65 - ETA: 1:14 - loss: 0.5882 - accuracy: 0.65 - ETA: 1:14 - loss: 0.5877 - accuracy: 0.65 - ETA: 1:14 - loss: 0.5867 - accuracy: 0.65 - ETA: 1:14 - loss: 0.5863 - accuracy: 0.65 - ETA: 1:14 - loss: 0.5853 - accuracy: 0.65 - ETA: 1:13 - loss: 0.5842 - accuracy: 0.65 - ETA: 1:13 - loss: 0.5836 - accuracy: 0.65 - ETA: 1:13 - loss: 0.5825 - accuracy: 0.66 - ETA: 1:13 - loss: 0.5819 - accuracy: 0.66 - ETA: 1:13 - loss: 0.5813 - accuracy: 0.66 - ETA: 1:13 - loss: 0.5811 - accuracy: 0.66 - ETA: 1:12 - loss: 0.5804 - accuracy: 0.66 - ETA: 1:12 - loss: 0.5804 - accuracy: 0.66 - ETA: 1:12 - loss: 0.5800 - accuracy: 0.66 - ETA: 1:12 - loss: 0.5794 - accuracy: 0.66 - ETA: 1:12 - loss: 0.5790 - accuracy: 0.66 - ETA: 1:12 - loss: 0.5777 - accuracy: 0.66 - ETA: 1:12 - loss: 0.5769 - accuracy: 0.66 - ETA: 1:12 - loss: 0.5762 - accuracy: 0.66 - ETA: 1:11 - loss: 0.5751 - accuracy: 0.66 - ETA: 1:11 - loss: 0.5739 - accuracy: 0.67 - ETA: 1:11 - loss: 0.5744 - accuracy: 0.66 - ETA: 1:11 - loss: 0.5739 - accuracy: 0.67 - ETA: 1:11 - loss: 0.5736 - accuracy: 0.67 - ETA: 1:11 - loss: 0.5741 - accuracy: 0.67 - ETA: 1:11 - loss: 0.5740 - accuracy: 0.67 - ETA: 1:11 - loss: 0.5740 - accuracy: 0.67 - ETA: 1:11 - loss: 0.5740 - accuracy: 0.67 - ETA: 1:10 - loss: 0.5737 - accuracy: 0.67 - ETA: 1:10 - loss: 0.5739 - accuracy: 0.67 - ETA: 1:10 - loss: 0.5736 - accuracy: 0.67 - ETA: 1:10 - loss: 0.5737 - accuracy: 0.67 - ETA: 1:10 - loss: 0.5733 - accuracy: 0.67 - ETA: 1:09 - loss: 0.5728 - accuracy: 0.67 - ETA: 1:09 - loss: 0.5727 - accuracy: 0.67 - ETA: 1:09 - loss: 0.5721 - accuracy: 0.67 - ETA: 1:09 - loss: 0.5715 - accuracy: 0.67 - ETA: 1:09 - loss: 0.5712 - accuracy: 0.67 - ETA: 1:09 - loss: 0.5703 - accuracy: 0.6754"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 667/2000 [=========>....................] - ETA: 1:09 - loss: 0.5702 - accuracy: 0.67 - ETA: 1:08 - loss: 0.5692 - accuracy: 0.67 - ETA: 1:08 - loss: 0.5692 - accuracy: 0.67 - ETA: 1:08 - loss: 0.5685 - accuracy: 0.67 - ETA: 1:08 - loss: 0.5680 - accuracy: 0.67 - ETA: 1:08 - loss: 0.5675 - accuracy: 0.67 - ETA: 1:08 - loss: 0.5674 - accuracy: 0.67 - ETA: 1:07 - loss: 0.5669 - accuracy: 0.67 - ETA: 1:07 - loss: 0.5667 - accuracy: 0.67 - ETA: 1:07 - loss: 0.5663 - accuracy: 0.67 - ETA: 1:07 - loss: 0.5660 - accuracy: 0.67 - ETA: 1:07 - loss: 0.5655 - accuracy: 0.67 - ETA: 1:07 - loss: 0.5652 - accuracy: 0.67 - ETA: 1:06 - loss: 0.5649 - accuracy: 0.67 - ETA: 1:06 - loss: 0.5640 - accuracy: 0.68 - ETA: 1:06 - loss: 0.5635 - accuracy: 0.68 - ETA: 1:06 - loss: 0.5629 - accuracy: 0.68 - ETA: 1:06 - loss: 0.5621 - accuracy: 0.68 - ETA: 1:06 - loss: 0.5619 - accuracy: 0.68 - ETA: 1:06 - loss: 0.5622 - accuracy: 0.68 - ETA: 1:05 - loss: 0.5620 - accuracy: 0.68 - ETA: 1:05 - loss: 0.5616 - accuracy: 0.68 - ETA: 1:05 - loss: 0.5613 - accuracy: 0.68 - ETA: 1:05 - loss: 0.5611 - accuracy: 0.68 - ETA: 1:05 - loss: 0.5608 - accuracy: 0.68 - ETA: 1:05 - loss: 0.5608 - accuracy: 0.68 - ETA: 1:05 - loss: 0.5601 - accuracy: 0.68 - ETA: 1:05 - loss: 0.5598 - accuracy: 0.68 - ETA: 1:05 - loss: 0.5594 - accuracy: 0.68 - ETA: 1:04 - loss: 0.5587 - accuracy: 0.68 - ETA: 1:04 - loss: 0.5583 - accuracy: 0.68 - ETA: 1:04 - loss: 0.5579 - accuracy: 0.68 - ETA: 1:04 - loss: 0.5575 - accuracy: 0.68 - ETA: 1:04 - loss: 0.5575 - accuracy: 0.68 - ETA: 1:04 - loss: 0.5571 - accuracy: 0.68 - ETA: 1:04 - loss: 0.5571 - accuracy: 0.68 - ETA: 1:04 - loss: 0.5566 - accuracy: 0.68 - ETA: 1:04 - loss: 0.5562 - accuracy: 0.68 - ETA: 1:04 - loss: 0.5555 - accuracy: 0.68 - ETA: 1:04 - loss: 0.5553 - accuracy: 0.68 - ETA: 1:04 - loss: 0.5548 - accuracy: 0.68 - ETA: 1:03 - loss: 0.5545 - accuracy: 0.68 - ETA: 1:03 - loss: 0.5543 - accuracy: 0.68 - ETA: 1:03 - loss: 0.5540 - accuracy: 0.68 - ETA: 1:03 - loss: 0.5538 - accuracy: 0.68 - ETA: 1:03 - loss: 0.5539 - accuracy: 0.68 - ETA: 1:03 - loss: 0.5536 - accuracy: 0.68 - ETA: 1:03 - loss: 0.5532 - accuracy: 0.68 - ETA: 1:03 - loss: 0.5528 - accuracy: 0.68 - ETA: 1:02 - loss: 0.5523 - accuracy: 0.68 - ETA: 1:02 - loss: 0.5519 - accuracy: 0.69 - ETA: 1:02 - loss: 0.5514 - accuracy: 0.69 - ETA: 1:02 - loss: 0.5510 - accuracy: 0.69 - ETA: 1:02 - loss: 0.5506 - accuracy: 0.69 - ETA: 1:02 - loss: 0.5501 - accuracy: 0.69 - ETA: 1:02 - loss: 0.5504 - accuracy: 0.69 - ETA: 1:02 - loss: 0.5497 - accuracy: 0.69 - ETA: 1:02 - loss: 0.5496 - accuracy: 0.69 - ETA: 1:01 - loss: 0.5493 - accuracy: 0.69 - ETA: 1:01 - loss: 0.5491 - accuracy: 0.69 - ETA: 1:01 - loss: 0.5484 - accuracy: 0.69 - ETA: 1:01 - loss: 0.5484 - accuracy: 0.69 - ETA: 1:01 - loss: 0.5480 - accuracy: 0.69 - ETA: 1:01 - loss: 0.5475 - accuracy: 0.69 - ETA: 1:01 - loss: 0.5468 - accuracy: 0.69 - ETA: 1:01 - loss: 0.5464 - accuracy: 0.69 - ETA: 1:01 - loss: 0.5463 - accuracy: 0.69 - ETA: 1:01 - loss: 0.5460 - accuracy: 0.69 - ETA: 1:01 - loss: 0.5457 - accuracy: 0.69 - ETA: 1:01 - loss: 0.5451 - accuracy: 0.69 - ETA: 1:01 - loss: 0.5448 - accuracy: 0.69 - ETA: 1:01 - loss: 0.5447 - accuracy: 0.69 - ETA: 1:01 - loss: 0.5442 - accuracy: 0.69 - ETA: 1:00 - loss: 0.5438 - accuracy: 0.69 - ETA: 1:00 - loss: 0.5433 - accuracy: 0.69 - ETA: 1:00 - loss: 0.5430 - accuracy: 0.69 - ETA: 1:00 - loss: 0.5428 - accuracy: 0.69 - ETA: 1:00 - loss: 0.5425 - accuracy: 0.69 - ETA: 1:00 - loss: 0.5421 - accuracy: 0.69 - ETA: 1:00 - loss: 0.5418 - accuracy: 0.69 - ETA: 1:00 - loss: 0.5411 - accuracy: 0.69 - ETA: 1:00 - loss: 0.5412 - accuracy: 0.69 - ETA: 1:00 - loss: 0.5412 - accuracy: 0.69 - ETA: 1:00 - loss: 0.5411 - accuracy: 0.69 - ETA: 1:00 - loss: 0.5409 - accuracy: 0.69 - ETA: 1:00 - loss: 0.5406 - accuracy: 0.70 - ETA: 1:00 - loss: 0.5401 - accuracy: 0.70 - ETA: 1:00 - loss: 0.5402 - accuracy: 0.70 - ETA: 1:00 - loss: 0.5396 - accuracy: 0.70 - ETA: 1:00 - loss: 0.5396 - accuracy: 0.70 - ETA: 1:00 - loss: 0.5396 - accuracy: 0.70 - ETA: 1:00 - loss: 0.5392 - accuracy: 0.70 - ETA: 1:00 - loss: 0.5390 - accuracy: 0.70 - ETA: 1:00 - loss: 0.5391 - accuracy: 0.70 - ETA: 59s - loss: 0.5390 - accuracy: 0.7016 - ETA: 59s - loss: 0.5388 - accuracy: 0.701 - ETA: 59s - loss: 0.5389 - accuracy: 0.701 - ETA: 59s - loss: 0.5384 - accuracy: 0.702 - ETA: 59s - loss: 0.5379 - accuracy: 0.702 - ETA: 59s - loss: 0.5374 - accuracy: 0.702 - ETA: 59s - loss: 0.5368 - accuracy: 0.703 - ETA: 59s - loss: 0.5366 - accuracy: 0.703 - ETA: 59s - loss: 0.5363 - accuracy: 0.703 - ETA: 59s - loss: 0.5357 - accuracy: 0.704 - ETA: 58s - loss: 0.5359 - accuracy: 0.704 - ETA: 58s - loss: 0.5359 - accuracy: 0.704 - ETA: 58s - loss: 0.5360 - accuracy: 0.704 - ETA: 58s - loss: 0.5359 - accuracy: 0.704 - ETA: 58s - loss: 0.5357 - accuracy: 0.704 - ETA: 58s - loss: 0.5355 - accuracy: 0.704 - ETA: 58s - loss: 0.5348 - accuracy: 0.704 - ETA: 58s - loss: 0.5344 - accuracy: 0.705 - ETA: 58s - loss: 0.5341 - accuracy: 0.705 - ETA: 58s - loss: 0.5339 - accuracy: 0.705 - ETA: 58s - loss: 0.5339 - accuracy: 0.705 - ETA: 58s - loss: 0.5337 - accuracy: 0.706 - ETA: 58s - loss: 0.5334 - accuracy: 0.706 - ETA: 58s - loss: 0.5333 - accuracy: 0.706 - ETA: 58s - loss: 0.5333 - accuracy: 0.706 - ETA: 58s - loss: 0.5332 - accuracy: 0.706 - ETA: 58s - loss: 0.5331 - accuracy: 0.706 - ETA: 58s - loss: 0.5330 - accuracy: 0.706 - ETA: 58s - loss: 0.5329 - accuracy: 0.706 - ETA: 58s - loss: 0.5329 - accuracy: 0.706 - ETA: 58s - loss: 0.5327 - accuracy: 0.706 - ETA: 58s - loss: 0.5326 - accuracy: 0.706 - ETA: 57s - loss: 0.5323 - accuracy: 0.706 - ETA: 57s - loss: 0.5319 - accuracy: 0.707 - ETA: 57s - loss: 0.5318 - accuracy: 0.707 - ETA: 57s - loss: 0.5315 - accuracy: 0.707 - ETA: 57s - loss: 0.5310 - accuracy: 0.708 - ETA: 57s - loss: 0.5310 - accuracy: 0.708 - ETA: 57s - loss: 0.5309 - accuracy: 0.708 - ETA: 57s - loss: 0.5309 - accuracy: 0.708 - ETA: 57s - loss: 0.5306 - accuracy: 0.708 - ETA: 57s - loss: 0.5304 - accuracy: 0.708 - ETA: 57s - loss: 0.5298 - accuracy: 0.709 - ETA: 57s - loss: 0.5295 - accuracy: 0.709 - ETA: 57s - loss: 0.5290 - accuracy: 0.709 - ETA: 57s - loss: 0.5285 - accuracy: 0.710 - ETA: 57s - loss: 0.5285 - accuracy: 0.710 - ETA: 56s - loss: 0.5280 - accuracy: 0.710 - ETA: 56s - loss: 0.5276 - accuracy: 0.711 - ETA: 56s - loss: 0.5271 - accuracy: 0.711 - ETA: 56s - loss: 0.5273 - accuracy: 0.711 - ETA: 56s - loss: 0.5273 - accuracy: 0.711 - ETA: 56s - loss: 0.5274 - accuracy: 0.711 - ETA: 56s - loss: 0.5275 - accuracy: 0.711 - ETA: 56s - loss: 0.5272 - accuracy: 0.711 - ETA: 56s - loss: 0.5269 - accuracy: 0.712 - ETA: 56s - loss: 0.5266 - accuracy: 0.712 - ETA: 55s - loss: 0.5266 - accuracy: 0.712 - ETA: 55s - loss: 0.5264 - accuracy: 0.712 - ETA: 55s - loss: 0.5263 - accuracy: 0.712 - ETA: 55s - loss: 0.5260 - accuracy: 0.712 - ETA: 55s - loss: 0.5259 - accuracy: 0.712 - ETA: 55s - loss: 0.5261 - accuracy: 0.712 - ETA: 55s - loss: 0.5258 - accuracy: 0.713 - ETA: 55s - loss: 0.5257 - accuracy: 0.712 - ETA: 55s - loss: 0.5256 - accuracy: 0.713 - ETA: 55s - loss: 0.5256 - accuracy: 0.713 - ETA: 55s - loss: 0.5256 - accuracy: 0.713 - ETA: 54s - loss: 0.5255 - accuracy: 0.713 - ETA: 54s - loss: 0.5256 - accuracy: 0.713 - ETA: 54s - loss: 0.5253 - accuracy: 0.713 - ETA: 54s - loss: 0.5251 - accuracy: 0.713 - ETA: 54s - loss: 0.5247 - accuracy: 0.713 - ETA: 54s - loss: 0.5247 - accuracy: 0.713 - ETA: 54s - loss: 0.5246 - accuracy: 0.713 - ETA: 54s - loss: 0.5244 - accuracy: 0.714 - ETA: 54s - loss: 0.5244 - accuracy: 0.714 - ETA: 54s - loss: 0.5245 - accuracy: 0.714 - ETA: 54s - loss: 0.5241 - accuracy: 0.714 - ETA: 54s - loss: 0.5238 - accuracy: 0.714 - ETA: 54s - loss: 0.5237 - accuracy: 0.714 - ETA: 54s - loss: 0.5236 - accuracy: 0.714 - ETA: 54s - loss: 0.5237 - accuracy: 0.714 - ETA: 54s - loss: 0.5235 - accuracy: 0.714 - ETA: 54s - loss: 0.5232 - accuracy: 0.714 - ETA: 54s - loss: 0.5231 - accuracy: 0.715 - ETA: 54s - loss: 0.5232 - accuracy: 0.714 - ETA: 54s - loss: 0.5229 - accuracy: 0.715 - ETA: 53s - loss: 0.5227 - accuracy: 0.715 - ETA: 53s - loss: 0.5225 - accuracy: 0.715 - ETA: 53s - loss: 0.5224 - accuracy: 0.716 - ETA: 53s - loss: 0.5222 - accuracy: 0.7162"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 994/2000 [=============>................] - ETA: 53s - loss: 0.5222 - accuracy: 0.716 - ETA: 53s - loss: 0.5221 - accuracy: 0.716 - ETA: 53s - loss: 0.5218 - accuracy: 0.716 - ETA: 53s - loss: 0.5219 - accuracy: 0.716 - ETA: 53s - loss: 0.5220 - accuracy: 0.716 - ETA: 53s - loss: 0.5220 - accuracy: 0.716 - ETA: 53s - loss: 0.5221 - accuracy: 0.716 - ETA: 53s - loss: 0.5220 - accuracy: 0.716 - ETA: 53s - loss: 0.5220 - accuracy: 0.716 - ETA: 53s - loss: 0.5221 - accuracy: 0.716 - ETA: 53s - loss: 0.5222 - accuracy: 0.716 - ETA: 53s - loss: 0.5221 - accuracy: 0.716 - ETA: 53s - loss: 0.5220 - accuracy: 0.716 - ETA: 52s - loss: 0.5217 - accuracy: 0.717 - ETA: 52s - loss: 0.5216 - accuracy: 0.717 - ETA: 52s - loss: 0.5216 - accuracy: 0.717 - ETA: 52s - loss: 0.5214 - accuracy: 0.717 - ETA: 52s - loss: 0.5215 - accuracy: 0.717 - ETA: 52s - loss: 0.5212 - accuracy: 0.717 - ETA: 52s - loss: 0.5212 - accuracy: 0.717 - ETA: 52s - loss: 0.5208 - accuracy: 0.718 - ETA: 52s - loss: 0.5204 - accuracy: 0.718 - ETA: 52s - loss: 0.5203 - accuracy: 0.718 - ETA: 51s - loss: 0.5201 - accuracy: 0.718 - ETA: 51s - loss: 0.5203 - accuracy: 0.718 - ETA: 51s - loss: 0.5201 - accuracy: 0.718 - ETA: 51s - loss: 0.5200 - accuracy: 0.718 - ETA: 51s - loss: 0.5201 - accuracy: 0.718 - ETA: 51s - loss: 0.5200 - accuracy: 0.718 - ETA: 51s - loss: 0.5199 - accuracy: 0.718 - ETA: 51s - loss: 0.5196 - accuracy: 0.718 - ETA: 51s - loss: 0.5195 - accuracy: 0.718 - ETA: 51s - loss: 0.5193 - accuracy: 0.718 - ETA: 51s - loss: 0.5192 - accuracy: 0.718 - ETA: 51s - loss: 0.5191 - accuracy: 0.719 - ETA: 50s - loss: 0.5190 - accuracy: 0.719 - ETA: 50s - loss: 0.5188 - accuracy: 0.719 - ETA: 50s - loss: 0.5187 - accuracy: 0.719 - ETA: 50s - loss: 0.5188 - accuracy: 0.719 - ETA: 50s - loss: 0.5188 - accuracy: 0.719 - ETA: 50s - loss: 0.5187 - accuracy: 0.719 - ETA: 50s - loss: 0.5186 - accuracy: 0.719 - ETA: 50s - loss: 0.5183 - accuracy: 0.719 - ETA: 50s - loss: 0.5184 - accuracy: 0.719 - ETA: 50s - loss: 0.5180 - accuracy: 0.719 - ETA: 50s - loss: 0.5179 - accuracy: 0.719 - ETA: 49s - loss: 0.5176 - accuracy: 0.720 - ETA: 49s - loss: 0.5175 - accuracy: 0.720 - ETA: 49s - loss: 0.5174 - accuracy: 0.720 - ETA: 49s - loss: 0.5169 - accuracy: 0.720 - ETA: 49s - loss: 0.5169 - accuracy: 0.720 - ETA: 49s - loss: 0.5165 - accuracy: 0.721 - ETA: 49s - loss: 0.5171 - accuracy: 0.721 - ETA: 49s - loss: 0.5173 - accuracy: 0.720 - ETA: 49s - loss: 0.5171 - accuracy: 0.721 - ETA: 49s - loss: 0.5170 - accuracy: 0.721 - ETA: 48s - loss: 0.5169 - accuracy: 0.721 - ETA: 48s - loss: 0.5170 - accuracy: 0.721 - ETA: 48s - loss: 0.5168 - accuracy: 0.721 - ETA: 48s - loss: 0.5168 - accuracy: 0.721 - ETA: 48s - loss: 0.5169 - accuracy: 0.721 - ETA: 48s - loss: 0.5169 - accuracy: 0.721 - ETA: 48s - loss: 0.5168 - accuracy: 0.721 - ETA: 48s - loss: 0.5169 - accuracy: 0.721 - ETA: 48s - loss: 0.5167 - accuracy: 0.721 - ETA: 48s - loss: 0.5166 - accuracy: 0.721 - ETA: 48s - loss: 0.5166 - accuracy: 0.721 - ETA: 48s - loss: 0.5166 - accuracy: 0.721 - ETA: 48s - loss: 0.5165 - accuracy: 0.721 - ETA: 48s - loss: 0.5165 - accuracy: 0.721 - ETA: 48s - loss: 0.5165 - accuracy: 0.721 - ETA: 47s - loss: 0.5165 - accuracy: 0.721 - ETA: 47s - loss: 0.5166 - accuracy: 0.721 - ETA: 47s - loss: 0.5166 - accuracy: 0.721 - ETA: 47s - loss: 0.5166 - accuracy: 0.721 - ETA: 47s - loss: 0.5167 - accuracy: 0.721 - ETA: 47s - loss: 0.5167 - accuracy: 0.721 - ETA: 47s - loss: 0.5166 - accuracy: 0.721 - ETA: 47s - loss: 0.5168 - accuracy: 0.720 - ETA: 47s - loss: 0.5166 - accuracy: 0.721 - ETA: 47s - loss: 0.5167 - accuracy: 0.721 - ETA: 47s - loss: 0.5167 - accuracy: 0.721 - ETA: 47s - loss: 0.5166 - accuracy: 0.721 - ETA: 47s - loss: 0.5166 - accuracy: 0.721 - ETA: 47s - loss: 0.5165 - accuracy: 0.721 - ETA: 47s - loss: 0.5164 - accuracy: 0.721 - ETA: 47s - loss: 0.5165 - accuracy: 0.721 - ETA: 47s - loss: 0.5164 - accuracy: 0.721 - ETA: 47s - loss: 0.5164 - accuracy: 0.721 - ETA: 47s - loss: 0.5163 - accuracy: 0.721 - ETA: 47s - loss: 0.5163 - accuracy: 0.721 - ETA: 47s - loss: 0.5162 - accuracy: 0.721 - ETA: 47s - loss: 0.5160 - accuracy: 0.721 - ETA: 47s - loss: 0.5158 - accuracy: 0.721 - ETA: 46s - loss: 0.5155 - accuracy: 0.722 - ETA: 46s - loss: 0.5153 - accuracy: 0.722 - ETA: 46s - loss: 0.5151 - accuracy: 0.722 - ETA: 46s - loss: 0.5149 - accuracy: 0.722 - ETA: 46s - loss: 0.5147 - accuracy: 0.722 - ETA: 46s - loss: 0.5146 - accuracy: 0.722 - ETA: 46s - loss: 0.5149 - accuracy: 0.722 - ETA: 46s - loss: 0.5146 - accuracy: 0.723 - ETA: 46s - loss: 0.5144 - accuracy: 0.723 - ETA: 46s - loss: 0.5141 - accuracy: 0.723 - ETA: 46s - loss: 0.5139 - accuracy: 0.723 - ETA: 45s - loss: 0.5139 - accuracy: 0.723 - ETA: 45s - loss: 0.5137 - accuracy: 0.723 - ETA: 45s - loss: 0.5137 - accuracy: 0.723 - ETA: 45s - loss: 0.5138 - accuracy: 0.723 - ETA: 45s - loss: 0.5137 - accuracy: 0.723 - ETA: 45s - loss: 0.5136 - accuracy: 0.723 - ETA: 45s - loss: 0.5135 - accuracy: 0.723 - ETA: 45s - loss: 0.5135 - accuracy: 0.723 - ETA: 45s - loss: 0.5133 - accuracy: 0.724 - ETA: 45s - loss: 0.5133 - accuracy: 0.724 - ETA: 45s - loss: 0.5133 - accuracy: 0.724 - ETA: 44s - loss: 0.5136 - accuracy: 0.724 - ETA: 44s - loss: 0.5134 - accuracy: 0.724 - ETA: 44s - loss: 0.5132 - accuracy: 0.724 - ETA: 44s - loss: 0.5132 - accuracy: 0.724 - ETA: 44s - loss: 0.5132 - accuracy: 0.724 - ETA: 44s - loss: 0.5132 - accuracy: 0.724 - ETA: 44s - loss: 0.5132 - accuracy: 0.724 - ETA: 44s - loss: 0.5132 - accuracy: 0.724 - ETA: 44s - loss: 0.5131 - accuracy: 0.724 - ETA: 44s - loss: 0.5130 - accuracy: 0.724 - ETA: 44s - loss: 0.5129 - accuracy: 0.724 - ETA: 44s - loss: 0.5128 - accuracy: 0.724 - ETA: 43s - loss: 0.5128 - accuracy: 0.724 - ETA: 43s - loss: 0.5128 - accuracy: 0.724 - ETA: 43s - loss: 0.5129 - accuracy: 0.724 - ETA: 43s - loss: 0.5129 - accuracy: 0.724 - ETA: 43s - loss: 0.5127 - accuracy: 0.724 - ETA: 43s - loss: 0.5128 - accuracy: 0.724 - ETA: 43s - loss: 0.5127 - accuracy: 0.724 - ETA: 43s - loss: 0.5125 - accuracy: 0.724 - ETA: 43s - loss: 0.5125 - accuracy: 0.724 - ETA: 43s - loss: 0.5124 - accuracy: 0.724 - ETA: 43s - loss: 0.5122 - accuracy: 0.725 - ETA: 43s - loss: 0.5123 - accuracy: 0.725 - ETA: 43s - loss: 0.5122 - accuracy: 0.725 - ETA: 43s - loss: 0.5121 - accuracy: 0.725 - ETA: 43s - loss: 0.5122 - accuracy: 0.725 - ETA: 43s - loss: 0.5122 - accuracy: 0.725 - ETA: 43s - loss: 0.5123 - accuracy: 0.724 - ETA: 43s - loss: 0.5123 - accuracy: 0.725 - ETA: 42s - loss: 0.5123 - accuracy: 0.725 - ETA: 42s - loss: 0.5123 - accuracy: 0.724 - ETA: 42s - loss: 0.5124 - accuracy: 0.724 - ETA: 42s - loss: 0.5123 - accuracy: 0.725 - ETA: 42s - loss: 0.5122 - accuracy: 0.725 - ETA: 42s - loss: 0.5123 - accuracy: 0.724 - ETA: 42s - loss: 0.5122 - accuracy: 0.725 - ETA: 42s - loss: 0.5120 - accuracy: 0.725 - ETA: 42s - loss: 0.5120 - accuracy: 0.725 - ETA: 42s - loss: 0.5118 - accuracy: 0.725 - ETA: 42s - loss: 0.5119 - accuracy: 0.725 - ETA: 42s - loss: 0.5117 - accuracy: 0.725 - ETA: 42s - loss: 0.5117 - accuracy: 0.725 - ETA: 41s - loss: 0.5116 - accuracy: 0.725 - ETA: 41s - loss: 0.5117 - accuracy: 0.725 - ETA: 41s - loss: 0.5115 - accuracy: 0.725 - ETA: 41s - loss: 0.5116 - accuracy: 0.725 - ETA: 41s - loss: 0.5115 - accuracy: 0.725 - ETA: 41s - loss: 0.5114 - accuracy: 0.725 - ETA: 41s - loss: 0.5113 - accuracy: 0.725 - ETA: 41s - loss: 0.5115 - accuracy: 0.725 - ETA: 41s - loss: 0.5113 - accuracy: 0.725 - ETA: 41s - loss: 0.5112 - accuracy: 0.725 - ETA: 41s - loss: 0.5110 - accuracy: 0.726 - ETA: 41s - loss: 0.5110 - accuracy: 0.726 - ETA: 41s - loss: 0.5107 - accuracy: 0.726 - ETA: 41s - loss: 0.5108 - accuracy: 0.726 - ETA: 41s - loss: 0.5105 - accuracy: 0.726 - ETA: 40s - loss: 0.5105 - accuracy: 0.726 - ETA: 40s - loss: 0.5104 - accuracy: 0.726 - ETA: 40s - loss: 0.5104 - accuracy: 0.726 - ETA: 40s - loss: 0.5102 - accuracy: 0.726 - ETA: 40s - loss: 0.5103 - accuracy: 0.726 - ETA: 40s - loss: 0.5102 - accuracy: 0.726 - ETA: 40s - loss: 0.5101 - accuracy: 0.726 - ETA: 40s - loss: 0.5099 - accuracy: 0.727 - ETA: 40s - loss: 0.5099 - accuracy: 0.727 - ETA: 40s - loss: 0.5098 - accuracy: 0.727 - ETA: 40s - loss: 0.5098 - accuracy: 0.727 - ETA: 39s - loss: 0.5095 - accuracy: 0.7273"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1339/2000 [===================>..........] - ETA: 39s - loss: 0.5095 - accuracy: 0.727 - ETA: 39s - loss: 0.5094 - accuracy: 0.727 - ETA: 39s - loss: 0.5095 - accuracy: 0.727 - ETA: 39s - loss: 0.5093 - accuracy: 0.727 - ETA: 39s - loss: 0.5093 - accuracy: 0.727 - ETA: 39s - loss: 0.5094 - accuracy: 0.727 - ETA: 39s - loss: 0.5094 - accuracy: 0.727 - ETA: 39s - loss: 0.5091 - accuracy: 0.727 - ETA: 39s - loss: 0.5091 - accuracy: 0.727 - ETA: 39s - loss: 0.5090 - accuracy: 0.727 - ETA: 39s - loss: 0.5088 - accuracy: 0.727 - ETA: 39s - loss: 0.5087 - accuracy: 0.727 - ETA: 39s - loss: 0.5087 - accuracy: 0.727 - ETA: 38s - loss: 0.5088 - accuracy: 0.727 - ETA: 38s - loss: 0.5086 - accuracy: 0.727 - ETA: 38s - loss: 0.5085 - accuracy: 0.727 - ETA: 38s - loss: 0.5085 - accuracy: 0.728 - ETA: 38s - loss: 0.5084 - accuracy: 0.728 - ETA: 38s - loss: 0.5085 - accuracy: 0.727 - ETA: 38s - loss: 0.5085 - accuracy: 0.727 - ETA: 38s - loss: 0.5084 - accuracy: 0.727 - ETA: 38s - loss: 0.5083 - accuracy: 0.727 - ETA: 38s - loss: 0.5082 - accuracy: 0.727 - ETA: 38s - loss: 0.5081 - accuracy: 0.728 - ETA: 38s - loss: 0.5079 - accuracy: 0.728 - ETA: 37s - loss: 0.5077 - accuracy: 0.728 - ETA: 37s - loss: 0.5076 - accuracy: 0.728 - ETA: 37s - loss: 0.5075 - accuracy: 0.728 - ETA: 37s - loss: 0.5076 - accuracy: 0.728 - ETA: 37s - loss: 0.5076 - accuracy: 0.728 - ETA: 37s - loss: 0.5076 - accuracy: 0.728 - ETA: 37s - loss: 0.5075 - accuracy: 0.728 - ETA: 37s - loss: 0.5074 - accuracy: 0.728 - ETA: 37s - loss: 0.5073 - accuracy: 0.728 - ETA: 37s - loss: 0.5073 - accuracy: 0.728 - ETA: 37s - loss: 0.5072 - accuracy: 0.728 - ETA: 37s - loss: 0.5071 - accuracy: 0.728 - ETA: 37s - loss: 0.5071 - accuracy: 0.728 - ETA: 36s - loss: 0.5070 - accuracy: 0.728 - ETA: 36s - loss: 0.5070 - accuracy: 0.728 - ETA: 36s - loss: 0.5069 - accuracy: 0.728 - ETA: 36s - loss: 0.5067 - accuracy: 0.728 - ETA: 36s - loss: 0.5067 - accuracy: 0.728 - ETA: 36s - loss: 0.5066 - accuracy: 0.729 - ETA: 36s - loss: 0.5064 - accuracy: 0.729 - ETA: 36s - loss: 0.5062 - accuracy: 0.729 - ETA: 36s - loss: 0.5062 - accuracy: 0.729 - ETA: 36s - loss: 0.5061 - accuracy: 0.729 - ETA: 36s - loss: 0.5062 - accuracy: 0.729 - ETA: 36s - loss: 0.5060 - accuracy: 0.729 - ETA: 35s - loss: 0.5061 - accuracy: 0.729 - ETA: 35s - loss: 0.5061 - accuracy: 0.729 - ETA: 35s - loss: 0.5062 - accuracy: 0.729 - ETA: 35s - loss: 0.5061 - accuracy: 0.729 - ETA: 35s - loss: 0.5061 - accuracy: 0.729 - ETA: 35s - loss: 0.5061 - accuracy: 0.729 - ETA: 35s - loss: 0.5061 - accuracy: 0.729 - ETA: 35s - loss: 0.5059 - accuracy: 0.729 - ETA: 35s - loss: 0.5059 - accuracy: 0.729 - ETA: 35s - loss: 0.5057 - accuracy: 0.729 - ETA: 35s - loss: 0.5057 - accuracy: 0.729 - ETA: 35s - loss: 0.5055 - accuracy: 0.729 - ETA: 34s - loss: 0.5054 - accuracy: 0.730 - ETA: 34s - loss: 0.5053 - accuracy: 0.730 - ETA: 34s - loss: 0.5051 - accuracy: 0.730 - ETA: 34s - loss: 0.5051 - accuracy: 0.730 - ETA: 34s - loss: 0.5047 - accuracy: 0.730 - ETA: 34s - loss: 0.5047 - accuracy: 0.730 - ETA: 34s - loss: 0.5045 - accuracy: 0.730 - ETA: 34s - loss: 0.5044 - accuracy: 0.731 - ETA: 34s - loss: 0.5047 - accuracy: 0.730 - ETA: 34s - loss: 0.5046 - accuracy: 0.730 - ETA: 34s - loss: 0.5044 - accuracy: 0.731 - ETA: 33s - loss: 0.5042 - accuracy: 0.731 - ETA: 33s - loss: 0.5041 - accuracy: 0.731 - ETA: 33s - loss: 0.5040 - accuracy: 0.731 - ETA: 33s - loss: 0.5039 - accuracy: 0.731 - ETA: 33s - loss: 0.5038 - accuracy: 0.731 - ETA: 33s - loss: 0.5038 - accuracy: 0.731 - ETA: 33s - loss: 0.5038 - accuracy: 0.731 - ETA: 33s - loss: 0.5037 - accuracy: 0.731 - ETA: 33s - loss: 0.5035 - accuracy: 0.731 - ETA: 33s - loss: 0.5034 - accuracy: 0.731 - ETA: 33s - loss: 0.5034 - accuracy: 0.731 - ETA: 33s - loss: 0.5033 - accuracy: 0.731 - ETA: 32s - loss: 0.5032 - accuracy: 0.732 - ETA: 32s - loss: 0.5031 - accuracy: 0.732 - ETA: 32s - loss: 0.5030 - accuracy: 0.732 - ETA: 32s - loss: 0.5029 - accuracy: 0.732 - ETA: 32s - loss: 0.5028 - accuracy: 0.732 - ETA: 32s - loss: 0.5027 - accuracy: 0.732 - ETA: 32s - loss: 0.5025 - accuracy: 0.732 - ETA: 32s - loss: 0.5025 - accuracy: 0.732 - ETA: 32s - loss: 0.5025 - accuracy: 0.732 - ETA: 32s - loss: 0.5026 - accuracy: 0.732 - ETA: 32s - loss: 0.5025 - accuracy: 0.732 - ETA: 32s - loss: 0.5025 - accuracy: 0.732 - ETA: 31s - loss: 0.5023 - accuracy: 0.732 - ETA: 31s - loss: 0.5023 - accuracy: 0.732 - ETA: 31s - loss: 0.5022 - accuracy: 0.732 - ETA: 31s - loss: 0.5021 - accuracy: 0.732 - ETA: 31s - loss: 0.5020 - accuracy: 0.733 - ETA: 31s - loss: 0.5019 - accuracy: 0.733 - ETA: 31s - loss: 0.5017 - accuracy: 0.733 - ETA: 31s - loss: 0.5016 - accuracy: 0.733 - ETA: 31s - loss: 0.5014 - accuracy: 0.733 - ETA: 31s - loss: 0.5013 - accuracy: 0.733 - ETA: 31s - loss: 0.5010 - accuracy: 0.733 - ETA: 31s - loss: 0.5009 - accuracy: 0.733 - ETA: 31s - loss: 0.5010 - accuracy: 0.733 - ETA: 30s - loss: 0.5009 - accuracy: 0.733 - ETA: 30s - loss: 0.5008 - accuracy: 0.733 - ETA: 30s - loss: 0.5007 - accuracy: 0.734 - ETA: 30s - loss: 0.5007 - accuracy: 0.734 - ETA: 30s - loss: 0.5006 - accuracy: 0.734 - ETA: 30s - loss: 0.5005 - accuracy: 0.734 - ETA: 30s - loss: 0.5004 - accuracy: 0.734 - ETA: 30s - loss: 0.5004 - accuracy: 0.734 - ETA: 30s - loss: 0.5003 - accuracy: 0.734 - ETA: 30s - loss: 0.5003 - accuracy: 0.734 - ETA: 30s - loss: 0.5003 - accuracy: 0.734 - ETA: 29s - loss: 0.5002 - accuracy: 0.734 - ETA: 29s - loss: 0.5003 - accuracy: 0.734 - ETA: 29s - loss: 0.5002 - accuracy: 0.734 - ETA: 29s - loss: 0.5001 - accuracy: 0.734 - ETA: 29s - loss: 0.5001 - accuracy: 0.734 - ETA: 29s - loss: 0.5000 - accuracy: 0.734 - ETA: 29s - loss: 0.5000 - accuracy: 0.734 - ETA: 29s - loss: 0.4998 - accuracy: 0.734 - ETA: 29s - loss: 0.4999 - accuracy: 0.734 - ETA: 29s - loss: 0.4999 - accuracy: 0.734 - ETA: 29s - loss: 0.4998 - accuracy: 0.734 - ETA: 29s - loss: 0.4998 - accuracy: 0.734 - ETA: 28s - loss: 0.4996 - accuracy: 0.734 - ETA: 28s - loss: 0.4994 - accuracy: 0.735 - ETA: 28s - loss: 0.4994 - accuracy: 0.735 - ETA: 28s - loss: 0.4994 - accuracy: 0.735 - ETA: 28s - loss: 0.4994 - accuracy: 0.735 - ETA: 28s - loss: 0.4995 - accuracy: 0.734 - ETA: 28s - loss: 0.4996 - accuracy: 0.734 - ETA: 28s - loss: 0.4996 - accuracy: 0.734 - ETA: 28s - loss: 0.4995 - accuracy: 0.734 - ETA: 28s - loss: 0.4995 - accuracy: 0.734 - ETA: 28s - loss: 0.4994 - accuracy: 0.734 - ETA: 28s - loss: 0.4992 - accuracy: 0.735 - ETA: 28s - loss: 0.4992 - accuracy: 0.735 - ETA: 28s - loss: 0.4993 - accuracy: 0.735 - ETA: 28s - loss: 0.4991 - accuracy: 0.735 - ETA: 28s - loss: 0.4992 - accuracy: 0.734 - ETA: 28s - loss: 0.4992 - accuracy: 0.735 - ETA: 27s - loss: 0.4992 - accuracy: 0.734 - ETA: 27s - loss: 0.4992 - accuracy: 0.734 - ETA: 27s - loss: 0.4992 - accuracy: 0.734 - ETA: 27s - loss: 0.4991 - accuracy: 0.735 - ETA: 27s - loss: 0.4991 - accuracy: 0.735 - ETA: 27s - loss: 0.4992 - accuracy: 0.735 - ETA: 27s - loss: 0.4992 - accuracy: 0.735 - ETA: 27s - loss: 0.4991 - accuracy: 0.735 - ETA: 27s - loss: 0.4991 - accuracy: 0.735 - ETA: 27s - loss: 0.4990 - accuracy: 0.735 - ETA: 27s - loss: 0.4989 - accuracy: 0.735 - ETA: 27s - loss: 0.4989 - accuracy: 0.735 - ETA: 27s - loss: 0.4988 - accuracy: 0.735 - ETA: 27s - loss: 0.4987 - accuracy: 0.735 - ETA: 27s - loss: 0.4986 - accuracy: 0.735 - ETA: 27s - loss: 0.4987 - accuracy: 0.735 - ETA: 26s - loss: 0.4986 - accuracy: 0.735 - ETA: 26s - loss: 0.4985 - accuracy: 0.735 - ETA: 26s - loss: 0.4983 - accuracy: 0.735 - ETA: 26s - loss: 0.4982 - accuracy: 0.735 - ETA: 26s - loss: 0.4982 - accuracy: 0.735 - ETA: 26s - loss: 0.4982 - accuracy: 0.735 - ETA: 26s - loss: 0.4981 - accuracy: 0.735 - ETA: 26s - loss: 0.4982 - accuracy: 0.735 - ETA: 26s - loss: 0.4981 - accuracy: 0.735 - ETA: 26s - loss: 0.4980 - accuracy: 0.736 - ETA: 26s - loss: 0.4980 - accuracy: 0.736 - ETA: 26s - loss: 0.4979 - accuracy: 0.736 - ETA: 26s - loss: 0.4977 - accuracy: 0.736 - ETA: 26s - loss: 0.4978 - accuracy: 0.736 - ETA: 26s - loss: 0.4977 - accuracy: 0.736 - ETA: 26s - loss: 0.4976 - accuracy: 0.736 - ETA: 26s - loss: 0.4976 - accuracy: 0.736 - ETA: 25s - loss: 0.4976 - accuracy: 0.736 - ETA: 25s - loss: 0.4975 - accuracy: 0.736 - ETA: 25s - loss: 0.4975 - accuracy: 0.7363"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1688/2000 [========================>.....] - ETA: 25s - loss: 0.4973 - accuracy: 0.736 - ETA: 25s - loss: 0.4973 - accuracy: 0.736 - ETA: 25s - loss: 0.4973 - accuracy: 0.736 - ETA: 25s - loss: 0.4973 - accuracy: 0.736 - ETA: 25s - loss: 0.4972 - accuracy: 0.736 - ETA: 25s - loss: 0.4972 - accuracy: 0.736 - ETA: 25s - loss: 0.4972 - accuracy: 0.736 - ETA: 25s - loss: 0.4972 - accuracy: 0.736 - ETA: 25s - loss: 0.4971 - accuracy: 0.736 - ETA: 25s - loss: 0.4971 - accuracy: 0.736 - ETA: 24s - loss: 0.4971 - accuracy: 0.736 - ETA: 24s - loss: 0.4970 - accuracy: 0.736 - ETA: 24s - loss: 0.4971 - accuracy: 0.736 - ETA: 24s - loss: 0.4970 - accuracy: 0.736 - ETA: 24s - loss: 0.4969 - accuracy: 0.736 - ETA: 24s - loss: 0.4969 - accuracy: 0.736 - ETA: 24s - loss: 0.4968 - accuracy: 0.736 - ETA: 24s - loss: 0.4968 - accuracy: 0.736 - ETA: 24s - loss: 0.4968 - accuracy: 0.736 - ETA: 24s - loss: 0.4967 - accuracy: 0.736 - ETA: 24s - loss: 0.4966 - accuracy: 0.737 - ETA: 24s - loss: 0.4966 - accuracy: 0.737 - ETA: 23s - loss: 0.4967 - accuracy: 0.737 - ETA: 23s - loss: 0.4968 - accuracy: 0.736 - ETA: 23s - loss: 0.4968 - accuracy: 0.736 - ETA: 23s - loss: 0.4968 - accuracy: 0.736 - ETA: 23s - loss: 0.4968 - accuracy: 0.736 - ETA: 23s - loss: 0.4967 - accuracy: 0.737 - ETA: 23s - loss: 0.4967 - accuracy: 0.737 - ETA: 23s - loss: 0.4967 - accuracy: 0.736 - ETA: 23s - loss: 0.4967 - accuracy: 0.736 - ETA: 23s - loss: 0.4966 - accuracy: 0.736 - ETA: 23s - loss: 0.4964 - accuracy: 0.737 - ETA: 23s - loss: 0.4964 - accuracy: 0.737 - ETA: 23s - loss: 0.4962 - accuracy: 0.737 - ETA: 22s - loss: 0.4961 - accuracy: 0.737 - ETA: 22s - loss: 0.4960 - accuracy: 0.737 - ETA: 22s - loss: 0.4959 - accuracy: 0.737 - ETA: 22s - loss: 0.4958 - accuracy: 0.737 - ETA: 22s - loss: 0.4956 - accuracy: 0.737 - ETA: 22s - loss: 0.4957 - accuracy: 0.737 - ETA: 22s - loss: 0.4958 - accuracy: 0.737 - ETA: 22s - loss: 0.4958 - accuracy: 0.737 - ETA: 22s - loss: 0.4958 - accuracy: 0.737 - ETA: 22s - loss: 0.4957 - accuracy: 0.737 - ETA: 22s - loss: 0.4957 - accuracy: 0.737 - ETA: 22s - loss: 0.4956 - accuracy: 0.737 - ETA: 21s - loss: 0.4955 - accuracy: 0.737 - ETA: 21s - loss: 0.4955 - accuracy: 0.738 - ETA: 21s - loss: 0.4954 - accuracy: 0.738 - ETA: 21s - loss: 0.4953 - accuracy: 0.738 - ETA: 21s - loss: 0.4950 - accuracy: 0.738 - ETA: 21s - loss: 0.4950 - accuracy: 0.738 - ETA: 21s - loss: 0.4949 - accuracy: 0.738 - ETA: 21s - loss: 0.4949 - accuracy: 0.738 - ETA: 21s - loss: 0.4948 - accuracy: 0.738 - ETA: 21s - loss: 0.4949 - accuracy: 0.738 - ETA: 21s - loss: 0.4949 - accuracy: 0.738 - ETA: 21s - loss: 0.4950 - accuracy: 0.738 - ETA: 20s - loss: 0.4950 - accuracy: 0.738 - ETA: 20s - loss: 0.4950 - accuracy: 0.738 - ETA: 20s - loss: 0.4949 - accuracy: 0.738 - ETA: 20s - loss: 0.4948 - accuracy: 0.738 - ETA: 20s - loss: 0.4947 - accuracy: 0.738 - ETA: 20s - loss: 0.4947 - accuracy: 0.738 - ETA: 20s - loss: 0.4945 - accuracy: 0.739 - ETA: 20s - loss: 0.4944 - accuracy: 0.739 - ETA: 20s - loss: 0.4944 - accuracy: 0.739 - ETA: 20s - loss: 0.4945 - accuracy: 0.739 - ETA: 20s - loss: 0.4944 - accuracy: 0.739 - ETA: 20s - loss: 0.4943 - accuracy: 0.739 - ETA: 20s - loss: 0.4943 - accuracy: 0.739 - ETA: 19s - loss: 0.4941 - accuracy: 0.739 - ETA: 19s - loss: 0.4941 - accuracy: 0.739 - ETA: 19s - loss: 0.4940 - accuracy: 0.739 - ETA: 19s - loss: 0.4940 - accuracy: 0.739 - ETA: 19s - loss: 0.4939 - accuracy: 0.739 - ETA: 19s - loss: 0.4939 - accuracy: 0.739 - ETA: 19s - loss: 0.4938 - accuracy: 0.739 - ETA: 19s - loss: 0.4937 - accuracy: 0.739 - ETA: 19s - loss: 0.4937 - accuracy: 0.739 - ETA: 19s - loss: 0.4937 - accuracy: 0.739 - ETA: 19s - loss: 0.4936 - accuracy: 0.739 - ETA: 19s - loss: 0.4935 - accuracy: 0.739 - ETA: 19s - loss: 0.4934 - accuracy: 0.739 - ETA: 18s - loss: 0.4934 - accuracy: 0.739 - ETA: 18s - loss: 0.4933 - accuracy: 0.740 - ETA: 18s - loss: 0.4931 - accuracy: 0.740 - ETA: 18s - loss: 0.4931 - accuracy: 0.740 - ETA: 18s - loss: 0.4931 - accuracy: 0.740 - ETA: 18s - loss: 0.4931 - accuracy: 0.740 - ETA: 18s - loss: 0.4930 - accuracy: 0.740 - ETA: 18s - loss: 0.4931 - accuracy: 0.740 - ETA: 18s - loss: 0.4930 - accuracy: 0.740 - ETA: 18s - loss: 0.4929 - accuracy: 0.740 - ETA: 18s - loss: 0.4929 - accuracy: 0.740 - ETA: 18s - loss: 0.4928 - accuracy: 0.740 - ETA: 18s - loss: 0.4927 - accuracy: 0.740 - ETA: 17s - loss: 0.4927 - accuracy: 0.740 - ETA: 17s - loss: 0.4926 - accuracy: 0.740 - ETA: 17s - loss: 0.4926 - accuracy: 0.740 - ETA: 17s - loss: 0.4927 - accuracy: 0.740 - ETA: 17s - loss: 0.4926 - accuracy: 0.740 - ETA: 17s - loss: 0.4927 - accuracy: 0.740 - ETA: 17s - loss: 0.4927 - accuracy: 0.740 - ETA: 17s - loss: 0.4928 - accuracy: 0.740 - ETA: 17s - loss: 0.4927 - accuracy: 0.740 - ETA: 17s - loss: 0.4926 - accuracy: 0.740 - ETA: 17s - loss: 0.4927 - accuracy: 0.740 - ETA: 17s - loss: 0.4926 - accuracy: 0.740 - ETA: 17s - loss: 0.4925 - accuracy: 0.740 - ETA: 17s - loss: 0.4926 - accuracy: 0.740 - ETA: 17s - loss: 0.4925 - accuracy: 0.740 - ETA: 16s - loss: 0.4925 - accuracy: 0.740 - ETA: 16s - loss: 0.4925 - accuracy: 0.740 - ETA: 16s - loss: 0.4925 - accuracy: 0.740 - ETA: 16s - loss: 0.4926 - accuracy: 0.740 - ETA: 16s - loss: 0.4925 - accuracy: 0.740 - ETA: 16s - loss: 0.4923 - accuracy: 0.740 - ETA: 16s - loss: 0.4923 - accuracy: 0.740 - ETA: 16s - loss: 0.4922 - accuracy: 0.740 - ETA: 16s - loss: 0.4921 - accuracy: 0.741 - ETA: 16s - loss: 0.4920 - accuracy: 0.741 - ETA: 16s - loss: 0.4920 - accuracy: 0.741 - ETA: 16s - loss: 0.4918 - accuracy: 0.741 - ETA: 16s - loss: 0.4919 - accuracy: 0.741 - ETA: 15s - loss: 0.4918 - accuracy: 0.741 - ETA: 15s - loss: 0.4919 - accuracy: 0.741 - ETA: 15s - loss: 0.4919 - accuracy: 0.741 - ETA: 15s - loss: 0.4920 - accuracy: 0.741 - ETA: 15s - loss: 0.4919 - accuracy: 0.741 - ETA: 15s - loss: 0.4919 - accuracy: 0.741 - ETA: 15s - loss: 0.4919 - accuracy: 0.741 - ETA: 15s - loss: 0.4919 - accuracy: 0.741 - ETA: 15s - loss: 0.4919 - accuracy: 0.741 - ETA: 15s - loss: 0.4918 - accuracy: 0.741 - ETA: 15s - loss: 0.4918 - accuracy: 0.741 - ETA: 15s - loss: 0.4917 - accuracy: 0.741 - ETA: 15s - loss: 0.4917 - accuracy: 0.741 - ETA: 15s - loss: 0.4916 - accuracy: 0.741 - ETA: 14s - loss: 0.4916 - accuracy: 0.741 - ETA: 14s - loss: 0.4916 - accuracy: 0.741 - ETA: 14s - loss: 0.4914 - accuracy: 0.741 - ETA: 14s - loss: 0.4913 - accuracy: 0.741 - ETA: 14s - loss: 0.4912 - accuracy: 0.741 - ETA: 14s - loss: 0.4911 - accuracy: 0.741 - ETA: 14s - loss: 0.4913 - accuracy: 0.741 - ETA: 14s - loss: 0.4913 - accuracy: 0.741 - ETA: 14s - loss: 0.4912 - accuracy: 0.741 - ETA: 14s - loss: 0.4911 - accuracy: 0.741 - ETA: 14s - loss: 0.4910 - accuracy: 0.741 - ETA: 14s - loss: 0.4910 - accuracy: 0.741 - ETA: 13s - loss: 0.4910 - accuracy: 0.741 - ETA: 13s - loss: 0.4909 - accuracy: 0.741 - ETA: 13s - loss: 0.4910 - accuracy: 0.741 - ETA: 13s - loss: 0.4910 - accuracy: 0.741 - ETA: 13s - loss: 0.4910 - accuracy: 0.741 - ETA: 13s - loss: 0.4909 - accuracy: 0.741 - ETA: 13s - loss: 0.4908 - accuracy: 0.741 - ETA: 13s - loss: 0.4909 - accuracy: 0.741 - ETA: 13s - loss: 0.4908 - accuracy: 0.741 - ETA: 13s - loss: 0.4909 - accuracy: 0.741 - ETA: 13s - loss: 0.4908 - accuracy: 0.741 - ETA: 13s - loss: 0.4908 - accuracy: 0.741 - ETA: 13s - loss: 0.4908 - accuracy: 0.741 - ETA: 13s - loss: 0.4908 - accuracy: 0.741 - ETA: 12s - loss: 0.4907 - accuracy: 0.741 - ETA: 12s - loss: 0.4908 - accuracy: 0.741 - ETA: 12s - loss: 0.4907 - accuracy: 0.741 - ETA: 12s - loss: 0.4907 - accuracy: 0.741 - ETA: 12s - loss: 0.4908 - accuracy: 0.741 - ETA: 12s - loss: 0.4908 - accuracy: 0.741 - ETA: 12s - loss: 0.4908 - accuracy: 0.741 - ETA: 12s - loss: 0.4907 - accuracy: 0.741 - ETA: 12s - loss: 0.4907 - accuracy: 0.741 - ETA: 12s - loss: 0.4907 - accuracy: 0.741 - ETA: 12s - loss: 0.4907 - accuracy: 0.741 - ETA: 12s - loss: 0.4906 - accuracy: 0.741 - ETA: 12s - loss: 0.4906 - accuracy: 0.741 - ETA: 12s - loss: 0.4906 - accuracy: 0.741 - ETA: 12s - loss: 0.4906 - accuracy: 0.741 - ETA: 12s - loss: 0.4906 - accuracy: 0.741 - ETA: 12s - loss: 0.4905 - accuracy: 0.741 - ETA: 12s - loss: 0.4906 - accuracy: 0.741 - ETA: 12s - loss: 0.4906 - accuracy: 0.741 - ETA: 12s - loss: 0.4904 - accuracy: 0.7419"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/2000 [==============================] - ETA: 11s - loss: 0.4903 - accuracy: 0.742 - ETA: 11s - loss: 0.4903 - accuracy: 0.742 - ETA: 11s - loss: 0.4902 - accuracy: 0.742 - ETA: 11s - loss: 0.4902 - accuracy: 0.742 - ETA: 11s - loss: 0.4902 - accuracy: 0.742 - ETA: 11s - loss: 0.4902 - accuracy: 0.742 - ETA: 11s - loss: 0.4902 - accuracy: 0.742 - ETA: 11s - loss: 0.4902 - accuracy: 0.742 - ETA: 11s - loss: 0.4901 - accuracy: 0.742 - ETA: 11s - loss: 0.4901 - accuracy: 0.742 - ETA: 11s - loss: 0.4899 - accuracy: 0.742 - ETA: 11s - loss: 0.4900 - accuracy: 0.742 - ETA: 10s - loss: 0.4899 - accuracy: 0.742 - ETA: 10s - loss: 0.4898 - accuracy: 0.742 - ETA: 10s - loss: 0.4896 - accuracy: 0.742 - ETA: 10s - loss: 0.4895 - accuracy: 0.742 - ETA: 10s - loss: 0.4896 - accuracy: 0.742 - ETA: 10s - loss: 0.4895 - accuracy: 0.742 - ETA: 10s - loss: 0.4894 - accuracy: 0.742 - ETA: 10s - loss: 0.4893 - accuracy: 0.742 - ETA: 10s - loss: 0.4894 - accuracy: 0.742 - ETA: 10s - loss: 0.4893 - accuracy: 0.742 - ETA: 10s - loss: 0.4893 - accuracy: 0.742 - ETA: 10s - loss: 0.4892 - accuracy: 0.743 - ETA: 10s - loss: 0.4893 - accuracy: 0.742 - ETA: 9s - loss: 0.4892 - accuracy: 0.743 - ETA: 9s - loss: 0.4892 - accuracy: 0.74 - ETA: 9s - loss: 0.4891 - accuracy: 0.74 - ETA: 9s - loss: 0.4891 - accuracy: 0.74 - ETA: 9s - loss: 0.4890 - accuracy: 0.74 - ETA: 9s - loss: 0.4889 - accuracy: 0.74 - ETA: 9s - loss: 0.4889 - accuracy: 0.74 - ETA: 9s - loss: 0.4889 - accuracy: 0.74 - ETA: 9s - loss: 0.4888 - accuracy: 0.74 - ETA: 9s - loss: 0.4888 - accuracy: 0.74 - ETA: 9s - loss: 0.4887 - accuracy: 0.74 - ETA: 9s - loss: 0.4886 - accuracy: 0.74 - ETA: 9s - loss: 0.4886 - accuracy: 0.74 - ETA: 8s - loss: 0.4886 - accuracy: 0.74 - ETA: 8s - loss: 0.4885 - accuracy: 0.74 - ETA: 8s - loss: 0.4884 - accuracy: 0.74 - ETA: 8s - loss: 0.4885 - accuracy: 0.74 - ETA: 8s - loss: 0.4884 - accuracy: 0.74 - ETA: 8s - loss: 0.4884 - accuracy: 0.74 - ETA: 8s - loss: 0.4884 - accuracy: 0.74 - ETA: 8s - loss: 0.4883 - accuracy: 0.74 - ETA: 8s - loss: 0.4882 - accuracy: 0.74 - ETA: 8s - loss: 0.4881 - accuracy: 0.74 - ETA: 8s - loss: 0.4879 - accuracy: 0.74 - ETA: 8s - loss: 0.4878 - accuracy: 0.74 - ETA: 8s - loss: 0.4877 - accuracy: 0.74 - ETA: 7s - loss: 0.4876 - accuracy: 0.74 - ETA: 7s - loss: 0.4875 - accuracy: 0.74 - ETA: 7s - loss: 0.4875 - accuracy: 0.74 - ETA: 7s - loss: 0.4875 - accuracy: 0.74 - ETA: 7s - loss: 0.4875 - accuracy: 0.74 - ETA: 7s - loss: 0.4875 - accuracy: 0.74 - ETA: 7s - loss: 0.4875 - accuracy: 0.74 - ETA: 7s - loss: 0.4875 - accuracy: 0.74 - ETA: 7s - loss: 0.4874 - accuracy: 0.74 - ETA: 7s - loss: 0.4873 - accuracy: 0.74 - ETA: 7s - loss: 0.4873 - accuracy: 0.74 - ETA: 7s - loss: 0.4872 - accuracy: 0.74 - ETA: 7s - loss: 0.4873 - accuracy: 0.74 - ETA: 6s - loss: 0.4872 - accuracy: 0.74 - ETA: 6s - loss: 0.4872 - accuracy: 0.74 - ETA: 6s - loss: 0.4871 - accuracy: 0.74 - ETA: 6s - loss: 0.4871 - accuracy: 0.74 - ETA: 6s - loss: 0.4871 - accuracy: 0.74 - ETA: 6s - loss: 0.4870 - accuracy: 0.74 - ETA: 6s - loss: 0.4870 - accuracy: 0.74 - ETA: 6s - loss: 0.4870 - accuracy: 0.74 - ETA: 6s - loss: 0.4870 - accuracy: 0.74 - ETA: 6s - loss: 0.4870 - accuracy: 0.74 - ETA: 6s - loss: 0.4870 - accuracy: 0.74 - ETA: 6s - loss: 0.4868 - accuracy: 0.74 - ETA: 6s - loss: 0.4869 - accuracy: 0.74 - ETA: 6s - loss: 0.4867 - accuracy: 0.74 - ETA: 5s - loss: 0.4868 - accuracy: 0.74 - ETA: 5s - loss: 0.4867 - accuracy: 0.74 - ETA: 5s - loss: 0.4866 - accuracy: 0.74 - ETA: 5s - loss: 0.4866 - accuracy: 0.74 - ETA: 5s - loss: 0.4864 - accuracy: 0.74 - ETA: 5s - loss: 0.4864 - accuracy: 0.74 - ETA: 5s - loss: 0.4865 - accuracy: 0.74 - ETA: 5s - loss: 0.4864 - accuracy: 0.74 - ETA: 5s - loss: 0.4865 - accuracy: 0.74 - ETA: 5s - loss: 0.4863 - accuracy: 0.74 - ETA: 5s - loss: 0.4863 - accuracy: 0.74 - ETA: 5s - loss: 0.4862 - accuracy: 0.74 - ETA: 5s - loss: 0.4863 - accuracy: 0.74 - ETA: 4s - loss: 0.4863 - accuracy: 0.74 - ETA: 4s - loss: 0.4862 - accuracy: 0.74 - ETA: 4s - loss: 0.4862 - accuracy: 0.74 - ETA: 4s - loss: 0.4861 - accuracy: 0.74 - ETA: 4s - loss: 0.4860 - accuracy: 0.74 - ETA: 4s - loss: 0.4860 - accuracy: 0.74 - ETA: 4s - loss: 0.4859 - accuracy: 0.74 - ETA: 4s - loss: 0.4860 - accuracy: 0.74 - ETA: 4s - loss: 0.4860 - accuracy: 0.74 - ETA: 4s - loss: 0.4858 - accuracy: 0.74 - ETA: 4s - loss: 0.4857 - accuracy: 0.74 - ETA: 4s - loss: 0.4857 - accuracy: 0.74 - ETA: 4s - loss: 0.4857 - accuracy: 0.74 - ETA: 4s - loss: 0.4857 - accuracy: 0.74 - ETA: 3s - loss: 0.4857 - accuracy: 0.74 - ETA: 3s - loss: 0.4857 - accuracy: 0.74 - ETA: 3s - loss: 0.4857 - accuracy: 0.74 - ETA: 3s - loss: 0.4857 - accuracy: 0.74 - ETA: 3s - loss: 0.4857 - accuracy: 0.74 - ETA: 3s - loss: 0.4857 - accuracy: 0.74 - ETA: 3s - loss: 0.4857 - accuracy: 0.74 - ETA: 3s - loss: 0.4856 - accuracy: 0.74 - ETA: 3s - loss: 0.4856 - accuracy: 0.74 - ETA: 3s - loss: 0.4856 - accuracy: 0.74 - ETA: 3s - loss: 0.4856 - accuracy: 0.74 - ETA: 3s - loss: 0.4856 - accuracy: 0.74 - ETA: 3s - loss: 0.4855 - accuracy: 0.74 - ETA: 3s - loss: 0.4855 - accuracy: 0.74 - ETA: 3s - loss: 0.4855 - accuracy: 0.74 - ETA: 3s - loss: 0.4855 - accuracy: 0.74 - ETA: 2s - loss: 0.4855 - accuracy: 0.74 - ETA: 2s - loss: 0.4855 - accuracy: 0.74 - ETA: 2s - loss: 0.4854 - accuracy: 0.74 - ETA: 2s - loss: 0.4854 - accuracy: 0.74 - ETA: 2s - loss: 0.4852 - accuracy: 0.74 - ETA: 2s - loss: 0.4852 - accuracy: 0.74 - ETA: 2s - loss: 0.4853 - accuracy: 0.74 - ETA: 2s - loss: 0.4852 - accuracy: 0.74 - ETA: 2s - loss: 0.4852 - accuracy: 0.74 - ETA: 2s - loss: 0.4851 - accuracy: 0.74 - ETA: 2s - loss: 0.4850 - accuracy: 0.74 - ETA: 2s - loss: 0.4850 - accuracy: 0.74 - ETA: 2s - loss: 0.4850 - accuracy: 0.74 - ETA: 2s - loss: 0.4849 - accuracy: 0.74 - ETA: 2s - loss: 0.4849 - accuracy: 0.74 - ETA: 1s - loss: 0.4848 - accuracy: 0.74 - ETA: 1s - loss: 0.4848 - accuracy: 0.74 - ETA: 1s - loss: 0.4848 - accuracy: 0.74 - ETA: 1s - loss: 0.4848 - accuracy: 0.74 - ETA: 1s - loss: 0.4847 - accuracy: 0.74 - ETA: 1s - loss: 0.4846 - accuracy: 0.74 - ETA: 1s - loss: 0.4846 - accuracy: 0.74 - ETA: 1s - loss: 0.4847 - accuracy: 0.74 - ETA: 1s - loss: 0.4846 - accuracy: 0.74 - ETA: 1s - loss: 0.4847 - accuracy: 0.74 - ETA: 1s - loss: 0.4847 - accuracy: 0.74 - ETA: 1s - loss: 0.4847 - accuracy: 0.74 - ETA: 1s - loss: 0.4851 - accuracy: 0.74 - ETA: 1s - loss: 0.4851 - accuracy: 0.74 - ETA: 1s - loss: 0.4850 - accuracy: 0.74 - ETA: 1s - loss: 0.4849 - accuracy: 0.74 - ETA: 1s - loss: 0.4851 - accuracy: 0.74 - ETA: 1s - loss: 0.4851 - accuracy: 0.74 - ETA: 1s - loss: 0.4850 - accuracy: 0.74 - ETA: 0s - loss: 0.4850 - accuracy: 0.74 - ETA: 0s - loss: 0.4850 - accuracy: 0.74 - ETA: 0s - loss: 0.4850 - accuracy: 0.74 - ETA: 0s - loss: 0.4850 - accuracy: 0.74 - ETA: 0s - loss: 0.4850 - accuracy: 0.74 - ETA: 0s - loss: 0.4850 - accuracy: 0.74 - ETA: 0s - loss: 0.4850 - accuracy: 0.74 - ETA: 0s - loss: 0.4851 - accuracy: 0.74 - ETA: 0s - loss: 0.4850 - accuracy: 0.74 - ETA: 0s - loss: 0.4850 - accuracy: 0.74 - ETA: 0s - loss: 0.4849 - accuracy: 0.74 - ETA: 0s - loss: 0.4850 - accuracy: 0.74 - ETA: 0s - loss: 0.4849 - accuracy: 0.74 - ETA: 0s - loss: 0.4849 - accuracy: 0.74 - ETA: 0s - loss: 0.4849 - accuracy: 0.74 - 81s 40ms/step - loss: 0.4850 - accuracy: 0.7467 - val_loss: 0.4831 - val_accuracy: 0.7479\n",
      "Epoch 2/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 339/2000 [====>.........................] - ETA: 1:59 - loss: 0.4142 - accuracy: 0.81 - ETA: 1:25 - loss: 0.4183 - accuracy: 0.80 - ETA: 1:17 - loss: 0.4138 - accuracy: 0.80 - ETA: 1:14 - loss: 0.4510 - accuracy: 0.77 - ETA: 1:11 - loss: 0.4712 - accuracy: 0.76 - ETA: 1:11 - loss: 0.4800 - accuracy: 0.74 - ETA: 1:11 - loss: 0.4898 - accuracy: 0.73 - ETA: 1:11 - loss: 0.4944 - accuracy: 0.72 - ETA: 1:11 - loss: 0.4947 - accuracy: 0.72 - ETA: 1:10 - loss: 0.5041 - accuracy: 0.72 - ETA: 1:10 - loss: 0.5030 - accuracy: 0.71 - ETA: 1:11 - loss: 0.5037 - accuracy: 0.71 - ETA: 1:11 - loss: 0.5013 - accuracy: 0.71 - ETA: 1:11 - loss: 0.5068 - accuracy: 0.71 - ETA: 1:11 - loss: 0.5091 - accuracy: 0.71 - ETA: 1:10 - loss: 0.5075 - accuracy: 0.71 - ETA: 1:11 - loss: 0.5054 - accuracy: 0.72 - ETA: 1:11 - loss: 0.5052 - accuracy: 0.72 - ETA: 1:11 - loss: 0.4970 - accuracy: 0.72 - ETA: 1:11 - loss: 0.4920 - accuracy: 0.73 - ETA: 1:11 - loss: 0.4903 - accuracy: 0.73 - ETA: 1:12 - loss: 0.4891 - accuracy: 0.73 - ETA: 1:12 - loss: 0.4919 - accuracy: 0.73 - ETA: 1:13 - loss: 0.4951 - accuracy: 0.73 - ETA: 1:13 - loss: 0.4964 - accuracy: 0.73 - ETA: 1:13 - loss: 0.4957 - accuracy: 0.73 - ETA: 1:13 - loss: 0.4964 - accuracy: 0.73 - ETA: 1:12 - loss: 0.4959 - accuracy: 0.73 - ETA: 1:12 - loss: 0.4947 - accuracy: 0.73 - ETA: 1:13 - loss: 0.4947 - accuracy: 0.73 - ETA: 1:13 - loss: 0.4930 - accuracy: 0.73 - ETA: 1:14 - loss: 0.4908 - accuracy: 0.73 - ETA: 1:13 - loss: 0.4916 - accuracy: 0.73 - ETA: 1:13 - loss: 0.4905 - accuracy: 0.73 - ETA: 1:13 - loss: 0.4893 - accuracy: 0.74 - ETA: 1:13 - loss: 0.4892 - accuracy: 0.74 - ETA: 1:14 - loss: 0.4890 - accuracy: 0.74 - ETA: 1:14 - loss: 0.4862 - accuracy: 0.74 - ETA: 1:14 - loss: 0.4851 - accuracy: 0.74 - ETA: 1:13 - loss: 0.4824 - accuracy: 0.74 - ETA: 1:13 - loss: 0.4826 - accuracy: 0.74 - ETA: 1:13 - loss: 0.4842 - accuracy: 0.74 - ETA: 1:13 - loss: 0.4835 - accuracy: 0.74 - ETA: 1:12 - loss: 0.4812 - accuracy: 0.74 - ETA: 1:12 - loss: 0.4814 - accuracy: 0.74 - ETA: 1:12 - loss: 0.4789 - accuracy: 0.75 - ETA: 1:12 - loss: 0.4796 - accuracy: 0.74 - ETA: 1:12 - loss: 0.4805 - accuracy: 0.74 - ETA: 1:11 - loss: 0.4808 - accuracy: 0.74 - ETA: 1:11 - loss: 0.4814 - accuracy: 0.74 - ETA: 1:11 - loss: 0.4830 - accuracy: 0.74 - ETA: 1:11 - loss: 0.4830 - accuracy: 0.74 - ETA: 1:11 - loss: 0.4820 - accuracy: 0.74 - ETA: 1:10 - loss: 0.4811 - accuracy: 0.74 - ETA: 1:10 - loss: 0.4805 - accuracy: 0.74 - ETA: 1:10 - loss: 0.4802 - accuracy: 0.74 - ETA: 1:10 - loss: 0.4803 - accuracy: 0.74 - ETA: 1:09 - loss: 0.4806 - accuracy: 0.74 - ETA: 1:09 - loss: 0.4792 - accuracy: 0.74 - ETA: 1:09 - loss: 0.4782 - accuracy: 0.74 - ETA: 1:09 - loss: 0.4771 - accuracy: 0.75 - ETA: 1:09 - loss: 0.4771 - accuracy: 0.75 - ETA: 1:09 - loss: 0.4771 - accuracy: 0.75 - ETA: 1:08 - loss: 0.4749 - accuracy: 0.75 - ETA: 1:08 - loss: 0.4755 - accuracy: 0.75 - ETA: 1:08 - loss: 0.4745 - accuracy: 0.75 - ETA: 1:08 - loss: 0.4734 - accuracy: 0.75 - ETA: 1:08 - loss: 0.4730 - accuracy: 0.75 - ETA: 1:08 - loss: 0.4727 - accuracy: 0.75 - ETA: 1:08 - loss: 0.4723 - accuracy: 0.75 - ETA: 1:07 - loss: 0.4724 - accuracy: 0.75 - ETA: 1:07 - loss: 0.4720 - accuracy: 0.75 - ETA: 1:07 - loss: 0.4708 - accuracy: 0.75 - ETA: 1:07 - loss: 0.4711 - accuracy: 0.75 - ETA: 1:07 - loss: 0.4706 - accuracy: 0.75 - ETA: 1:07 - loss: 0.4713 - accuracy: 0.75 - ETA: 1:07 - loss: 0.4717 - accuracy: 0.75 - ETA: 1:07 - loss: 0.4717 - accuracy: 0.75 - ETA: 1:07 - loss: 0.4725 - accuracy: 0.75 - ETA: 1:06 - loss: 0.4728 - accuracy: 0.75 - ETA: 1:06 - loss: 0.4736 - accuracy: 0.75 - ETA: 1:06 - loss: 0.4736 - accuracy: 0.75 - ETA: 1:06 - loss: 0.4730 - accuracy: 0.75 - ETA: 1:06 - loss: 0.4722 - accuracy: 0.75 - ETA: 1:07 - loss: 0.4723 - accuracy: 0.75 - ETA: 1:07 - loss: 0.4722 - accuracy: 0.75 - ETA: 1:07 - loss: 0.4726 - accuracy: 0.75 - ETA: 1:07 - loss: 0.4722 - accuracy: 0.75 - ETA: 1:07 - loss: 0.4721 - accuracy: 0.75 - ETA: 1:07 - loss: 0.4713 - accuracy: 0.75 - ETA: 1:07 - loss: 0.4721 - accuracy: 0.75 - ETA: 1:07 - loss: 0.4723 - accuracy: 0.75 - ETA: 1:07 - loss: 0.4716 - accuracy: 0.75 - ETA: 1:07 - loss: 0.4721 - accuracy: 0.75 - ETA: 1:07 - loss: 0.4723 - accuracy: 0.75 - ETA: 1:07 - loss: 0.4717 - accuracy: 0.75 - ETA: 1:07 - loss: 0.4718 - accuracy: 0.75 - ETA: 1:07 - loss: 0.4711 - accuracy: 0.75 - ETA: 1:07 - loss: 0.4716 - accuracy: 0.75 - ETA: 1:07 - loss: 0.4709 - accuracy: 0.75 - ETA: 1:07 - loss: 0.4715 - accuracy: 0.75 - ETA: 1:07 - loss: 0.4715 - accuracy: 0.75 - ETA: 1:07 - loss: 0.4714 - accuracy: 0.75 - ETA: 1:07 - loss: 0.4715 - accuracy: 0.75 - ETA: 1:07 - loss: 0.4714 - accuracy: 0.75 - ETA: 1:07 - loss: 0.4709 - accuracy: 0.75 - ETA: 1:07 - loss: 0.4712 - accuracy: 0.75 - ETA: 1:07 - loss: 0.4711 - accuracy: 0.75 - ETA: 1:06 - loss: 0.4714 - accuracy: 0.75 - ETA: 1:06 - loss: 0.4710 - accuracy: 0.75 - ETA: 1:06 - loss: 0.4703 - accuracy: 0.75 - ETA: 1:06 - loss: 0.4701 - accuracy: 0.75 - ETA: 1:06 - loss: 0.4695 - accuracy: 0.75 - ETA: 1:06 - loss: 0.4695 - accuracy: 0.75 - ETA: 1:06 - loss: 0.4689 - accuracy: 0.75 - ETA: 1:06 - loss: 0.4690 - accuracy: 0.75 - ETA: 1:06 - loss: 0.4686 - accuracy: 0.75 - ETA: 1:06 - loss: 0.4692 - accuracy: 0.75 - ETA: 1:06 - loss: 0.4692 - accuracy: 0.75 - ETA: 1:06 - loss: 0.4698 - accuracy: 0.75 - ETA: 1:05 - loss: 0.4696 - accuracy: 0.75 - ETA: 1:05 - loss: 0.4700 - accuracy: 0.75 - ETA: 1:05 - loss: 0.4697 - accuracy: 0.75 - ETA: 1:05 - loss: 0.4698 - accuracy: 0.75 - ETA: 1:05 - loss: 0.4689 - accuracy: 0.75 - ETA: 1:05 - loss: 0.4692 - accuracy: 0.75 - ETA: 1:05 - loss: 0.4687 - accuracy: 0.75 - ETA: 1:05 - loss: 0.4685 - accuracy: 0.75 - ETA: 1:05 - loss: 0.4679 - accuracy: 0.76 - ETA: 1:05 - loss: 0.4680 - accuracy: 0.76 - ETA: 1:04 - loss: 0.4683 - accuracy: 0.75 - ETA: 1:04 - loss: 0.4679 - accuracy: 0.76 - ETA: 1:04 - loss: 0.4687 - accuracy: 0.75 - ETA: 1:04 - loss: 0.4682 - accuracy: 0.75 - ETA: 1:04 - loss: 0.4677 - accuracy: 0.76 - ETA: 1:04 - loss: 0.4677 - accuracy: 0.76 - ETA: 1:04 - loss: 0.4673 - accuracy: 0.76 - ETA: 1:04 - loss: 0.4674 - accuracy: 0.76 - ETA: 1:04 - loss: 0.4673 - accuracy: 0.76 - ETA: 1:04 - loss: 0.4676 - accuracy: 0.76 - ETA: 1:04 - loss: 0.4682 - accuracy: 0.75 - ETA: 1:04 - loss: 0.4681 - accuracy: 0.76 - ETA: 1:04 - loss: 0.4686 - accuracy: 0.75 - ETA: 1:04 - loss: 0.4683 - accuracy: 0.75 - ETA: 1:04 - loss: 0.4677 - accuracy: 0.76 - ETA: 1:04 - loss: 0.4675 - accuracy: 0.76 - ETA: 1:04 - loss: 0.4672 - accuracy: 0.76 - ETA: 1:04 - loss: 0.4669 - accuracy: 0.76 - ETA: 1:04 - loss: 0.4668 - accuracy: 0.76 - ETA: 1:04 - loss: 0.4668 - accuracy: 0.76 - ETA: 1:04 - loss: 0.4662 - accuracy: 0.76 - ETA: 1:04 - loss: 0.4656 - accuracy: 0.76 - ETA: 1:03 - loss: 0.4659 - accuracy: 0.76 - ETA: 1:03 - loss: 0.4658 - accuracy: 0.76 - ETA: 1:03 - loss: 0.4654 - accuracy: 0.76 - ETA: 1:03 - loss: 0.4653 - accuracy: 0.76 - ETA: 1:03 - loss: 0.4656 - accuracy: 0.76 - ETA: 1:03 - loss: 0.4654 - accuracy: 0.76 - ETA: 1:03 - loss: 0.4657 - accuracy: 0.76 - ETA: 1:03 - loss: 0.4653 - accuracy: 0.76 - ETA: 1:03 - loss: 0.4650 - accuracy: 0.76 - ETA: 1:03 - loss: 0.4640 - accuracy: 0.76 - ETA: 1:03 - loss: 0.4643 - accuracy: 0.76 - ETA: 1:03 - loss: 0.4639 - accuracy: 0.76 - ETA: 1:03 - loss: 0.4638 - accuracy: 0.76 - ETA: 1:03 - loss: 0.4630 - accuracy: 0.76 - ETA: 1:03 - loss: 0.4634 - accuracy: 0.76 - ETA: 1:03 - loss: 0.4630 - accuracy: 0.76 - ETA: 1:02 - loss: 0.4635 - accuracy: 0.76 - ETA: 1:02 - loss: 0.4635 - accuracy: 0.76 - ETA: 1:02 - loss: 0.4634 - accuracy: 0.76 - ETA: 1:02 - loss: 0.4629 - accuracy: 0.76 - ETA: 1:02 - loss: 0.4627 - accuracy: 0.76 - ETA: 1:02 - loss: 0.4626 - accuracy: 0.76 - ETA: 1:02 - loss: 0.4625 - accuracy: 0.76 - ETA: 1:02 - loss: 0.4625 - accuracy: 0.76 - ETA: 1:02 - loss: 0.4626 - accuracy: 0.76 - ETA: 1:02 - loss: 0.4623 - accuracy: 0.76 - ETA: 1:02 - loss: 0.4621 - accuracy: 0.76 - ETA: 1:02 - loss: 0.4617 - accuracy: 0.76 - ETA: 1:02 - loss: 0.4619 - accuracy: 0.76 - ETA: 1:02 - loss: 0.4615 - accuracy: 0.76 - ETA: 1:02 - loss: 0.4611 - accuracy: 0.76 - ETA: 1:02 - loss: 0.4612 - accuracy: 0.76 - ETA: 1:02 - loss: 0.4612 - accuracy: 0.76 - ETA: 1:02 - loss: 0.4608 - accuracy: 0.7666"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 673/2000 [=========>....................] - ETA: 1:02 - loss: 0.4611 - accuracy: 0.76 - ETA: 1:02 - loss: 0.4612 - accuracy: 0.76 - ETA: 1:02 - loss: 0.4608 - accuracy: 0.76 - ETA: 1:02 - loss: 0.4612 - accuracy: 0.76 - ETA: 1:02 - loss: 0.4610 - accuracy: 0.76 - ETA: 1:02 - loss: 0.4609 - accuracy: 0.76 - ETA: 1:02 - loss: 0.4608 - accuracy: 0.76 - ETA: 1:02 - loss: 0.4611 - accuracy: 0.76 - ETA: 1:02 - loss: 0.4612 - accuracy: 0.76 - ETA: 1:02 - loss: 0.4614 - accuracy: 0.76 - ETA: 1:02 - loss: 0.4613 - accuracy: 0.76 - ETA: 1:02 - loss: 0.4613 - accuracy: 0.76 - ETA: 1:02 - loss: 0.4611 - accuracy: 0.76 - ETA: 1:02 - loss: 0.4610 - accuracy: 0.76 - ETA: 1:01 - loss: 0.4610 - accuracy: 0.76 - ETA: 1:01 - loss: 0.4611 - accuracy: 0.76 - ETA: 1:01 - loss: 0.4608 - accuracy: 0.76 - ETA: 1:01 - loss: 0.4606 - accuracy: 0.76 - ETA: 1:01 - loss: 0.4604 - accuracy: 0.76 - ETA: 1:01 - loss: 0.4602 - accuracy: 0.76 - ETA: 1:01 - loss: 0.4601 - accuracy: 0.76 - ETA: 1:01 - loss: 0.4603 - accuracy: 0.76 - ETA: 1:01 - loss: 0.4609 - accuracy: 0.76 - ETA: 1:01 - loss: 0.4608 - accuracy: 0.76 - ETA: 1:01 - loss: 0.4605 - accuracy: 0.76 - ETA: 1:00 - loss: 0.4605 - accuracy: 0.76 - ETA: 1:00 - loss: 0.4607 - accuracy: 0.76 - ETA: 1:00 - loss: 0.4606 - accuracy: 0.76 - ETA: 1:00 - loss: 0.4606 - accuracy: 0.76 - ETA: 1:00 - loss: 0.4606 - accuracy: 0.76 - ETA: 1:00 - loss: 0.4608 - accuracy: 0.76 - ETA: 1:00 - loss: 0.4604 - accuracy: 0.76 - ETA: 1:00 - loss: 0.4603 - accuracy: 0.76 - ETA: 1:00 - loss: 0.4602 - accuracy: 0.76 - ETA: 1:00 - loss: 0.4603 - accuracy: 0.76 - ETA: 59s - loss: 0.4604 - accuracy: 0.7666 - ETA: 59s - loss: 0.4606 - accuracy: 0.766 - ETA: 59s - loss: 0.4606 - accuracy: 0.766 - ETA: 59s - loss: 0.4605 - accuracy: 0.766 - ETA: 59s - loss: 0.4604 - accuracy: 0.766 - ETA: 59s - loss: 0.4603 - accuracy: 0.766 - ETA: 59s - loss: 0.4605 - accuracy: 0.766 - ETA: 59s - loss: 0.4603 - accuracy: 0.766 - ETA: 59s - loss: 0.4604 - accuracy: 0.766 - ETA: 59s - loss: 0.4607 - accuracy: 0.766 - ETA: 59s - loss: 0.4608 - accuracy: 0.766 - ETA: 59s - loss: 0.4608 - accuracy: 0.766 - ETA: 59s - loss: 0.4611 - accuracy: 0.766 - ETA: 59s - loss: 0.4611 - accuracy: 0.765 - ETA: 59s - loss: 0.4615 - accuracy: 0.765 - ETA: 59s - loss: 0.4616 - accuracy: 0.765 - ETA: 59s - loss: 0.4615 - accuracy: 0.765 - ETA: 59s - loss: 0.4614 - accuracy: 0.765 - ETA: 59s - loss: 0.4614 - accuracy: 0.765 - ETA: 58s - loss: 0.4612 - accuracy: 0.765 - ETA: 58s - loss: 0.4612 - accuracy: 0.765 - ETA: 58s - loss: 0.4610 - accuracy: 0.766 - ETA: 58s - loss: 0.4610 - accuracy: 0.766 - ETA: 58s - loss: 0.4609 - accuracy: 0.766 - ETA: 58s - loss: 0.4607 - accuracy: 0.766 - ETA: 58s - loss: 0.4611 - accuracy: 0.766 - ETA: 58s - loss: 0.4607 - accuracy: 0.766 - ETA: 58s - loss: 0.4610 - accuracy: 0.766 - ETA: 58s - loss: 0.4610 - accuracy: 0.766 - ETA: 58s - loss: 0.4612 - accuracy: 0.766 - ETA: 58s - loss: 0.4610 - accuracy: 0.766 - ETA: 58s - loss: 0.4610 - accuracy: 0.766 - ETA: 58s - loss: 0.4610 - accuracy: 0.766 - ETA: 57s - loss: 0.4608 - accuracy: 0.766 - ETA: 57s - loss: 0.4603 - accuracy: 0.767 - ETA: 57s - loss: 0.4603 - accuracy: 0.767 - ETA: 57s - loss: 0.4603 - accuracy: 0.767 - ETA: 57s - loss: 0.4603 - accuracy: 0.766 - ETA: 57s - loss: 0.4604 - accuracy: 0.766 - ETA: 57s - loss: 0.4600 - accuracy: 0.767 - ETA: 57s - loss: 0.4598 - accuracy: 0.767 - ETA: 57s - loss: 0.4596 - accuracy: 0.767 - ETA: 57s - loss: 0.4597 - accuracy: 0.767 - ETA: 56s - loss: 0.4592 - accuracy: 0.767 - ETA: 56s - loss: 0.4592 - accuracy: 0.767 - ETA: 56s - loss: 0.4592 - accuracy: 0.767 - ETA: 56s - loss: 0.4591 - accuracy: 0.767 - ETA: 56s - loss: 0.4590 - accuracy: 0.767 - ETA: 56s - loss: 0.4586 - accuracy: 0.768 - ETA: 56s - loss: 0.4591 - accuracy: 0.767 - ETA: 56s - loss: 0.4592 - accuracy: 0.767 - ETA: 56s - loss: 0.4590 - accuracy: 0.767 - ETA: 56s - loss: 0.4591 - accuracy: 0.767 - ETA: 56s - loss: 0.4592 - accuracy: 0.767 - ETA: 55s - loss: 0.4592 - accuracy: 0.767 - ETA: 55s - loss: 0.4591 - accuracy: 0.767 - ETA: 55s - loss: 0.4592 - accuracy: 0.767 - ETA: 55s - loss: 0.4590 - accuracy: 0.768 - ETA: 55s - loss: 0.4594 - accuracy: 0.767 - ETA: 55s - loss: 0.4593 - accuracy: 0.767 - ETA: 55s - loss: 0.4594 - accuracy: 0.767 - ETA: 55s - loss: 0.4595 - accuracy: 0.767 - ETA: 55s - loss: 0.4595 - accuracy: 0.767 - ETA: 55s - loss: 0.4593 - accuracy: 0.767 - ETA: 55s - loss: 0.4590 - accuracy: 0.768 - ETA: 55s - loss: 0.4586 - accuracy: 0.768 - ETA: 55s - loss: 0.4589 - accuracy: 0.768 - ETA: 54s - loss: 0.4588 - accuracy: 0.768 - ETA: 54s - loss: 0.4587 - accuracy: 0.768 - ETA: 54s - loss: 0.4589 - accuracy: 0.768 - ETA: 54s - loss: 0.4592 - accuracy: 0.768 - ETA: 54s - loss: 0.4593 - accuracy: 0.768 - ETA: 54s - loss: 0.4593 - accuracy: 0.768 - ETA: 54s - loss: 0.4592 - accuracy: 0.768 - ETA: 54s - loss: 0.4595 - accuracy: 0.767 - ETA: 54s - loss: 0.4590 - accuracy: 0.768 - ETA: 54s - loss: 0.4589 - accuracy: 0.768 - ETA: 54s - loss: 0.4588 - accuracy: 0.768 - ETA: 54s - loss: 0.4587 - accuracy: 0.768 - ETA: 54s - loss: 0.4589 - accuracy: 0.768 - ETA: 54s - loss: 0.4588 - accuracy: 0.768 - ETA: 54s - loss: 0.4586 - accuracy: 0.768 - ETA: 54s - loss: 0.4587 - accuracy: 0.768 - ETA: 54s - loss: 0.4588 - accuracy: 0.768 - ETA: 54s - loss: 0.4589 - accuracy: 0.768 - ETA: 53s - loss: 0.4590 - accuracy: 0.767 - ETA: 53s - loss: 0.4590 - accuracy: 0.767 - ETA: 53s - loss: 0.4591 - accuracy: 0.767 - ETA: 53s - loss: 0.4590 - accuracy: 0.767 - ETA: 53s - loss: 0.4588 - accuracy: 0.768 - ETA: 53s - loss: 0.4588 - accuracy: 0.768 - ETA: 53s - loss: 0.4587 - accuracy: 0.768 - ETA: 53s - loss: 0.4587 - accuracy: 0.768 - ETA: 53s - loss: 0.4589 - accuracy: 0.768 - ETA: 53s - loss: 0.4587 - accuracy: 0.768 - ETA: 53s - loss: 0.4587 - accuracy: 0.768 - ETA: 53s - loss: 0.4584 - accuracy: 0.768 - ETA: 53s - loss: 0.4583 - accuracy: 0.768 - ETA: 53s - loss: 0.4581 - accuracy: 0.769 - ETA: 53s - loss: 0.4579 - accuracy: 0.769 - ETA: 52s - loss: 0.4581 - accuracy: 0.768 - ETA: 52s - loss: 0.4579 - accuracy: 0.769 - ETA: 52s - loss: 0.4577 - accuracy: 0.769 - ETA: 52s - loss: 0.4576 - accuracy: 0.769 - ETA: 52s - loss: 0.4579 - accuracy: 0.769 - ETA: 52s - loss: 0.4579 - accuracy: 0.769 - ETA: 52s - loss: 0.4580 - accuracy: 0.769 - ETA: 52s - loss: 0.4581 - accuracy: 0.769 - ETA: 52s - loss: 0.4583 - accuracy: 0.769 - ETA: 52s - loss: 0.4581 - accuracy: 0.769 - ETA: 52s - loss: 0.4581 - accuracy: 0.769 - ETA: 52s - loss: 0.4580 - accuracy: 0.769 - ETA: 52s - loss: 0.4581 - accuracy: 0.769 - ETA: 52s - loss: 0.4580 - accuracy: 0.769 - ETA: 51s - loss: 0.4580 - accuracy: 0.769 - ETA: 51s - loss: 0.4580 - accuracy: 0.769 - ETA: 51s - loss: 0.4582 - accuracy: 0.768 - ETA: 51s - loss: 0.4583 - accuracy: 0.768 - ETA: 51s - loss: 0.4581 - accuracy: 0.768 - ETA: 51s - loss: 0.4583 - accuracy: 0.768 - ETA: 51s - loss: 0.4584 - accuracy: 0.768 - ETA: 51s - loss: 0.4587 - accuracy: 0.768 - ETA: 51s - loss: 0.4588 - accuracy: 0.768 - ETA: 51s - loss: 0.4589 - accuracy: 0.768 - ETA: 51s - loss: 0.4591 - accuracy: 0.767 - ETA: 50s - loss: 0.4589 - accuracy: 0.767 - ETA: 50s - loss: 0.4587 - accuracy: 0.768 - ETA: 50s - loss: 0.4587 - accuracy: 0.768 - ETA: 50s - loss: 0.4587 - accuracy: 0.768 - ETA: 50s - loss: 0.4589 - accuracy: 0.767 - ETA: 50s - loss: 0.4587 - accuracy: 0.768 - ETA: 50s - loss: 0.4586 - accuracy: 0.768 - ETA: 50s - loss: 0.4586 - accuracy: 0.768 - ETA: 50s - loss: 0.4587 - accuracy: 0.768 - ETA: 50s - loss: 0.4588 - accuracy: 0.767 - ETA: 50s - loss: 0.4587 - accuracy: 0.768 - ETA: 50s - loss: 0.4588 - accuracy: 0.767 - ETA: 50s - loss: 0.4587 - accuracy: 0.767 - ETA: 50s - loss: 0.4589 - accuracy: 0.767 - ETA: 50s - loss: 0.4587 - accuracy: 0.767 - ETA: 50s - loss: 0.4587 - accuracy: 0.768 - ETA: 50s - loss: 0.4586 - accuracy: 0.768 - ETA: 50s - loss: 0.4587 - accuracy: 0.768 - ETA: 50s - loss: 0.4585 - accuracy: 0.768 - ETA: 50s - loss: 0.4586 - accuracy: 0.768 - ETA: 50s - loss: 0.4586 - accuracy: 0.768 - ETA: 50s - loss: 0.4587 - accuracy: 0.768 - ETA: 50s - loss: 0.4587 - accuracy: 0.768 - ETA: 50s - loss: 0.4587 - accuracy: 0.768 - ETA: 50s - loss: 0.4588 - accuracy: 0.768 - ETA: 50s - loss: 0.4585 - accuracy: 0.7684"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 845/2000 [===========>..................] - ETA: 50s - loss: 0.4586 - accuracy: 0.768 - ETA: 50s - loss: 0.4588 - accuracy: 0.768 - ETA: 49s - loss: 0.4587 - accuracy: 0.768 - ETA: 49s - loss: 0.4588 - accuracy: 0.768 - ETA: 49s - loss: 0.4591 - accuracy: 0.767 - ETA: 49s - loss: 0.4592 - accuracy: 0.767 - ETA: 49s - loss: 0.4592 - accuracy: 0.767 - ETA: 49s - loss: 0.4592 - accuracy: 0.767 - ETA: 49s - loss: 0.4592 - accuracy: 0.767 - ETA: 49s - loss: 0.4592 - accuracy: 0.767 - ETA: 49s - loss: 0.4592 - accuracy: 0.767 - ETA: 49s - loss: 0.4594 - accuracy: 0.767 - ETA: 49s - loss: 0.4593 - accuracy: 0.767 - ETA: 48s - loss: 0.4593 - accuracy: 0.767 - ETA: 48s - loss: 0.4590 - accuracy: 0.768 - ETA: 48s - loss: 0.4588 - accuracy: 0.768 - ETA: 48s - loss: 0.4587 - accuracy: 0.768 - ETA: 48s - loss: 0.4588 - accuracy: 0.768 - ETA: 48s - loss: 0.4591 - accuracy: 0.768 - ETA: 48s - loss: 0.4590 - accuracy: 0.768 - ETA: 48s - loss: 0.4589 - accuracy: 0.768 - ETA: 48s - loss: 0.4592 - accuracy: 0.768 - ETA: 48s - loss: 0.4592 - accuracy: 0.767 - ETA: 48s - loss: 0.4592 - accuracy: 0.767 - ETA: 47s - loss: 0.4593 - accuracy: 0.767 - ETA: 47s - loss: 0.4591 - accuracy: 0.767 - ETA: 47s - loss: 0.4590 - accuracy: 0.767 - ETA: 47s - loss: 0.4590 - accuracy: 0.768 - ETA: 47s - loss: 0.4590 - accuracy: 0.768 - ETA: 47s - loss: 0.4590 - accuracy: 0.768 - ETA: 47s - loss: 0.4588 - accuracy: 0.768 - ETA: 47s - loss: 0.4591 - accuracy: 0.767 - ETA: 47s - loss: 0.4593 - accuracy: 0.767 - ETA: 47s - loss: 0.4592 - accuracy: 0.767 - ETA: 47s - loss: 0.4592 - accuracy: 0.767 - ETA: 47s - loss: 0.4592 - accuracy: 0.767 - ETA: 46s - loss: 0.4595 - accuracy: 0.767 - ETA: 46s - loss: 0.4593 - accuracy: 0.767 - ETA: 46s - loss: 0.4593 - accuracy: 0.767 - ETA: 46s - loss: 0.4592 - accuracy: 0.767 - ETA: 46s - loss: 0.4592 - accuracy: 0.767 - ETA: 46s - loss: 0.4592 - accuracy: 0.767 - ETA: 46s - loss: 0.4589 - accuracy: 0.767 - ETA: 46s - loss: 0.4590 - accuracy: 0.767 - ETA: 46s - loss: 0.4590 - accuracy: 0.767 - ETA: 46s - loss: 0.4589 - accuracy: 0.767 - ETA: 46s - loss: 0.4589 - accuracy: 0.768 - ETA: 46s - loss: 0.4592 - accuracy: 0.767 - ETA: 46s - loss: 0.4591 - accuracy: 0.767 - ETA: 46s - loss: 0.4591 - accuracy: 0.767 - ETA: 46s - loss: 0.4590 - accuracy: 0.767 - ETA: 46s - loss: 0.4589 - accuracy: 0.767 - ETA: 46s - loss: 0.4589 - accuracy: 0.768 - ETA: 46s - loss: 0.4589 - accuracy: 0.767 - ETA: 45s - loss: 0.4588 - accuracy: 0.768 - ETA: 45s - loss: 0.4590 - accuracy: 0.767 - ETA: 45s - loss: 0.4590 - accuracy: 0.767 - ETA: 46s - loss: 0.4591 - accuracy: 0.767 - ETA: 45s - loss: 0.4589 - accuracy: 0.767 - ETA: 45s - loss: 0.4589 - accuracy: 0.767 - ETA: 45s - loss: 0.4588 - accuracy: 0.767 - ETA: 45s - loss: 0.4588 - accuracy: 0.767 - ETA: 45s - loss: 0.4588 - accuracy: 0.767 - ETA: 45s - loss: 0.4588 - accuracy: 0.767 - ETA: 45s - loss: 0.4589 - accuracy: 0.767 - ETA: 45s - loss: 0.4587 - accuracy: 0.767 - ETA: 45s - loss: 0.4590 - accuracy: 0.767 - ETA: 45s - loss: 0.4590 - accuracy: 0.767 - ETA: 45s - loss: 0.4590 - accuracy: 0.767 - ETA: 45s - loss: 0.4590 - accuracy: 0.767 - ETA: 44s - loss: 0.4593 - accuracy: 0.767 - ETA: 44s - loss: 0.4592 - accuracy: 0.767 - ETA: 44s - loss: 0.4594 - accuracy: 0.767 - ETA: 44s - loss: 0.4594 - accuracy: 0.767 - ETA: 44s - loss: 0.4596 - accuracy: 0.766 - ETA: 44s - loss: 0.4595 - accuracy: 0.766 - ETA: 44s - loss: 0.4595 - accuracy: 0.766 - ETA: 44s - loss: 0.4595 - accuracy: 0.766 - ETA: 44s - loss: 0.4596 - accuracy: 0.766 - ETA: 44s - loss: 0.4597 - accuracy: 0.766 - ETA: 44s - loss: 0.4596 - accuracy: 0.766 - ETA: 44s - loss: 0.4596 - accuracy: 0.766 - ETA: 44s - loss: 0.4596 - accuracy: 0.767 - ETA: 44s - loss: 0.4596 - accuracy: 0.767 - ETA: 44s - loss: 0.4594 - accuracy: 0.767 - ETA: 44s - loss: 0.4593 - accuracy: 0.767 - ETA: 44s - loss: 0.4591 - accuracy: 0.767 - ETA: 44s - loss: 0.4590 - accuracy: 0.767 - ETA: 44s - loss: 0.4589 - accuracy: 0.767 - ETA: 44s - loss: 0.4589 - accuracy: 0.767 - ETA: 44s - loss: 0.4588 - accuracy: 0.767 - ETA: 44s - loss: 0.4587 - accuracy: 0.768 - ETA: 44s - loss: 0.4586 - accuracy: 0.768 - ETA: 44s - loss: 0.4587 - accuracy: 0.767 - ETA: 43s - loss: 0.4591 - accuracy: 0.767 - ETA: 43s - loss: 0.4589 - accuracy: 0.7677WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-9d46e9ea0e26>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mmax_trials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m )\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/autokeras/auto_model.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, callbacks, validation_split, validation_data, **kwargs)\u001b[0m\n\u001b[1;32m    256\u001b[0m                           \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m                           \u001b[0mfit_on_val_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_split_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 258\u001b[0;31m                           **kwargs)\n\u001b[0m\u001b[1;32m    259\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_process_x\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/autokeras/engine/tuner.py\u001b[0m in \u001b[0;36msearch\u001b[0;34m(self, callbacks, fit_on_val_data, **fit_kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0mnew_callbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEarlyStopping\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnew_callbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;31m# Fully train the best model with original callbacks.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/kerastuner/engine/base_tuner.py\u001b[0m in \u001b[0;36msearch\u001b[0;34m(self, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_trial_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mfit_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_trial_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_search_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/autokeras/engine/tuner.py\u001b[0m in \u001b[0;36mrun_trial\u001b[0;34m(self, trial, x, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhypermodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhyperparameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madapt_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m             \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mfit_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcopied_fit_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_values\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moracle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobjective\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirection\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'min'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    817\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    340\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m                 total_epochs=epochs)\n\u001b[0m\u001b[1;32m    343\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[0;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[1;32m    126\u001b[0m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[1;32m    127\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;31m# TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[0;34m(input_fn)\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;31m# `numpy` translates Tensors to values in Eager mode.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[0;32m---> 98\u001b[0;31m                               distributed_function(input_fn))\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    566\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 568\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    569\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    597\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 599\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    600\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2361\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2362\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2363\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2365\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1609\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1610\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1611\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1613\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1690\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1692\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1694\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    543\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"executor_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"config_proto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 545\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow_core/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[1;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_features = len(r._reber_letters) + 1 # include padding token\n",
    "# TODO: how to mask in the embedding\n",
    "input_node = ak.Input()\n",
    "output_node = ak.Embedding(max_features=num_features)(input_node)\n",
    "output_node = ak.RNNBlock(\n",
    "    bidirectional=False,\n",
    "    num_layers=1,\n",
    "    layer_type='gru'\n",
    ")(output_node)\n",
    "output_node = ak.ClassificationHead()(output_node)\n",
    "clf = ak.AutoModel(\n",
    "    inputs=input_node,\n",
    "    outputs=output_node,\n",
    "    max_trials=1\n",
    ")\n",
    "clf.fit(X_train.to_numpy(), y_train.to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 80000 samples, validate on 20000 samples\n",
      "Epoch 1/2\n",
      "80000/80000 [==============================] - 17s 217us/step - loss: 0.6903 - accuracy: 0.5657 - val_loss: 0.6851 - val_accuracy: 0.4983\n",
      "Epoch 2/2\n",
      "80000/80000 [==============================] - 18s 224us/step - loss: 0.6665 - accuracy: 0.5379 - val_loss: 0.6395 - val_accuracy: 0.6458\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    validation_data=(X_test, y_test),\n",
    "    epochs=1,\n",
    "    batch_size=5000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6.3349569e-01],\n",
       "       [6.1708295e-01],\n",
       "       [6.1269373e-01],\n",
       "       [6.2125689e-01],\n",
       "       [6.5021406e-05],\n",
       "       [6.4222826e-05]], dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "strs = [\n",
    "    \"BPBTSSXXVVEPE\", # true reber\n",
    "    \"BPBTSSSSXXVPXTTTVPSETE\", # second and second to last not the same, but reber inbetween\n",
    "    \"BPBPTTTTTVPBTTTTVPSEPT\", # second and second to last the same, but not reber in middle\n",
    "    \"BBBBBBBBBBB\",\n",
    "    \"XXXXXXXXXXXXXX\",\n",
    "    \"TTTTTTTTTTTTTTTTTTT\"\n",
    "]\n",
    "model.predict(\n",
    "    np.array([\n",
    "        r.encode_as_padded_ints(s) for s in strs\n",
    "    ])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly, XXXXXXXXXX is not in the reber grammar, and yet the model predicted with 98.6% certainty that it was. This means that I need to make my examples of non-reber much more random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.76853848"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "119879    0\n",
       "103694    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[-5:-3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
