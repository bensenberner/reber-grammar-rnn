{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Dense, LSTM, Dropout\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.models import Sequential\n",
    "import autokeras as ak\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import reber\n",
    "RANDOM_STATE = 42\n",
    "PADDING_IDX = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = reber.ReberGenerator(max_length=25)\n",
    "X, y = r.make_data(total_num_rows=6000)\n",
    "_, word_len = X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1, random_state=RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_vector_length = len(r._reber_letters) # TODO: what should this be?\n",
    "num_neurons = 60\n",
    "model = Sequential()\n",
    "Embedding\n",
    "model.add(\n",
    "    Embedding(\n",
    "        embedding_vector_length + 1,\n",
    "        embedding_vector_length,\n",
    "        input_length=word_len,\n",
    "        mask_zero=True\n",
    "    )\n",
    ")\n",
    "model.add(LSTM(num_neurons))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reloading Oracle from existing project ./auto_model/oracle.json\n",
      "INFO:tensorflow:Reloading Tuner from ./auto_model/tuner0.json\n",
      "INFO:tensorflow:Oracle triggered exit\n",
      "Train for 169 steps, validate for 34 steps\n",
      "Epoch 1/3\n",
      "169/169 [==============================] - ETA: 7:19 - loss: 0.6958 - accuracy: 0.46 - ETA: 3:43 - loss: 0.6963 - accuracy: 0.46 - ETA: 2:32 - loss: 0.6904 - accuracy: 0.54 - ETA: 1:56 - loss: 0.6862 - accuracy: 0.56 - ETA: 1:34 - loss: 0.6916 - accuracy: 0.53 - ETA: 1:19 - loss: 0.6894 - accuracy: 0.54 - ETA: 1:09 - loss: 0.6870 - accuracy: 0.54 - ETA: 1:01 - loss: 0.6873 - accuracy: 0.54 - ETA: 55s - loss: 0.6920 - accuracy: 0.5312 - ETA: 50s - loss: 0.6976 - accuracy: 0.512 - ETA: 46s - loss: 0.6962 - accuracy: 0.514 - ETA: 43s - loss: 0.6977 - accuracy: 0.507 - ETA: 40s - loss: 0.6977 - accuracy: 0.502 - ETA: 37s - loss: 0.6967 - accuracy: 0.522 - ETA: 35s - loss: 0.6963 - accuracy: 0.522 - ETA: 33s - loss: 0.6957 - accuracy: 0.527 - ETA: 32s - loss: 0.6957 - accuracy: 0.522 - ETA: 30s - loss: 0.6948 - accuracy: 0.524 - ETA: 29s - loss: 0.6940 - accuracy: 0.528 - ETA: 27s - loss: 0.6937 - accuracy: 0.526 - ETA: 26s - loss: 0.6940 - accuracy: 0.520 - ETA: 25s - loss: 0.6925 - accuracy: 0.528 - ETA: 24s - loss: 0.6932 - accuracy: 0.521 - ETA: 24s - loss: 0.6920 - accuracy: 0.527 - ETA: 23s - loss: 0.6913 - accuracy: 0.530 - ETA: 22s - loss: 0.6922 - accuracy: 0.522 - ETA: 22s - loss: 0.6912 - accuracy: 0.519 - ETA: 21s - loss: 0.6910 - accuracy: 0.516 - ETA: 20s - loss: 0.6898 - accuracy: 0.519 - ETA: 20s - loss: 0.6883 - accuracy: 0.526 - ETA: 20s - loss: 0.6862 - accuracy: 0.530 - ETA: 19s - loss: 0.6851 - accuracy: 0.532 - ETA: 19s - loss: 0.6840 - accuracy: 0.535 - ETA: 18s - loss: 0.6834 - accuracy: 0.534 - ETA: 18s - loss: 0.6835 - accuracy: 0.537 - ETA: 18s - loss: 0.6838 - accuracy: 0.536 - ETA: 17s - loss: 0.6846 - accuracy: 0.540 - ETA: 17s - loss: 0.6822 - accuracy: 0.543 - ETA: 16s - loss: 0.6834 - accuracy: 0.541 - ETA: 16s - loss: 0.6838 - accuracy: 0.540 - ETA: 16s - loss: 0.6838 - accuracy: 0.540 - ETA: 15s - loss: 0.6821 - accuracy: 0.543 - ETA: 15s - loss: 0.6823 - accuracy: 0.542 - ETA: 15s - loss: 0.6815 - accuracy: 0.545 - ETA: 15s - loss: 0.6813 - accuracy: 0.543 - ETA: 14s - loss: 0.6804 - accuracy: 0.547 - ETA: 14s - loss: 0.6801 - accuracy: 0.551 - ETA: 14s - loss: 0.6796 - accuracy: 0.554 - ETA: 14s - loss: 0.6789 - accuracy: 0.558 - ETA: 14s - loss: 0.6783 - accuracy: 0.560 - ETA: 13s - loss: 0.6772 - accuracy: 0.565 - ETA: 13s - loss: 0.6766 - accuracy: 0.566 - ETA: 13s - loss: 0.6754 - accuracy: 0.567 - ETA: 13s - loss: 0.6753 - accuracy: 0.567 - ETA: 12s - loss: 0.6747 - accuracy: 0.568 - ETA: 12s - loss: 0.6740 - accuracy: 0.569 - ETA: 12s - loss: 0.6733 - accuracy: 0.570 - ETA: 12s - loss: 0.6727 - accuracy: 0.571 - ETA: 12s - loss: 0.6726 - accuracy: 0.572 - ETA: 11s - loss: 0.6748 - accuracy: 0.570 - ETA: 11s - loss: 0.6740 - accuracy: 0.572 - ETA: 11s - loss: 0.6734 - accuracy: 0.571 - ETA: 11s - loss: 0.6717 - accuracy: 0.574 - ETA: 11s - loss: 0.6698 - accuracy: 0.576 - ETA: 10s - loss: 0.6694 - accuracy: 0.576 - ETA: 10s - loss: 0.6679 - accuracy: 0.579 - ETA: 10s - loss: 0.6677 - accuracy: 0.579 - ETA: 10s - loss: 0.6664 - accuracy: 0.581 - ETA: 10s - loss: 0.6661 - accuracy: 0.582 - ETA: 10s - loss: 0.6649 - accuracy: 0.585 - ETA: 9s - loss: 0.6641 - accuracy: 0.587 - ETA: 9s - loss: 0.6636 - accuracy: 0.58 - ETA: 9s - loss: 0.6619 - accuracy: 0.58 - ETA: 9s - loss: 0.6609 - accuracy: 0.59 - ETA: 9s - loss: 0.6601 - accuracy: 0.59 - ETA: 9s - loss: 0.6599 - accuracy: 0.59 - ETA: 8s - loss: 0.6611 - accuracy: 0.59 - ETA: 8s - loss: 0.6600 - accuracy: 0.59 - ETA: 8s - loss: 0.6598 - accuracy: 0.59 - ETA: 8s - loss: 0.6589 - accuracy: 0.59 - ETA: 8s - loss: 0.6576 - accuracy: 0.59 - ETA: 8s - loss: 0.6573 - accuracy: 0.59 - ETA: 8s - loss: 0.6569 - accuracy: 0.59 - ETA: 7s - loss: 0.6564 - accuracy: 0.59 - ETA: 7s - loss: 0.6546 - accuracy: 0.59 - ETA: 7s - loss: 0.6528 - accuracy: 0.60 - ETA: 7s - loss: 0.6519 - accuracy: 0.60 - ETA: 7s - loss: 0.6515 - accuracy: 0.60 - ETA: 7s - loss: 0.6510 - accuracy: 0.60 - ETA: 7s - loss: 0.6503 - accuracy: 0.60 - ETA: 7s - loss: 0.6492 - accuracy: 0.60 - ETA: 6s - loss: 0.6484 - accuracy: 0.60 - ETA: 6s - loss: 0.6487 - accuracy: 0.60 - ETA: 6s - loss: 0.6490 - accuracy: 0.60 - ETA: 6s - loss: 0.6472 - accuracy: 0.61 - ETA: 6s - loss: 0.6447 - accuracy: 0.61 - ETA: 6s - loss: 0.6441 - accuracy: 0.61 - ETA: 6s - loss: 0.6427 - accuracy: 0.61 - ETA: 6s - loss: 0.6424 - accuracy: 0.61 - ETA: 6s - loss: 0.6419 - accuracy: 0.61 - ETA: 5s - loss: 0.6410 - accuracy: 0.61 - ETA: 5s - loss: 0.6412 - accuracy: 0.61 - ETA: 5s - loss: 0.6394 - accuracy: 0.61 - ETA: 5s - loss: 0.6388 - accuracy: 0.61 - ETA: 5s - loss: 0.6380 - accuracy: 0.61 - ETA: 5s - loss: 0.6384 - accuracy: 0.61 - ETA: 5s - loss: 0.6377 - accuracy: 0.61 - ETA: 5s - loss: 0.6371 - accuracy: 0.61 - ETA: 5s - loss: 0.6366 - accuracy: 0.61 - ETA: 5s - loss: 0.6365 - accuracy: 0.61 - ETA: 4s - loss: 0.6369 - accuracy: 0.61 - ETA: 4s - loss: 0.6360 - accuracy: 0.61 - ETA: 4s - loss: 0.6357 - accuracy: 0.61 - ETA: 4s - loss: 0.6352 - accuracy: 0.61 - ETA: 4s - loss: 0.6346 - accuracy: 0.61 - ETA: 4s - loss: 0.6342 - accuracy: 0.61 - ETA: 4s - loss: 0.6331 - accuracy: 0.61 - ETA: 4s - loss: 0.6325 - accuracy: 0.62 - ETA: 4s - loss: 0.6312 - accuracy: 0.62 - ETA: 4s - loss: 0.6321 - accuracy: 0.62 - ETA: 4s - loss: 0.6311 - accuracy: 0.62 - ETA: 3s - loss: 0.6313 - accuracy: 0.62 - ETA: 3s - loss: 0.6308 - accuracy: 0.62 - ETA: 3s - loss: 0.6314 - accuracy: 0.62 - ETA: 3s - loss: 0.6303 - accuracy: 0.62 - ETA: 3s - loss: 0.6302 - accuracy: 0.62 - ETA: 3s - loss: 0.6305 - accuracy: 0.62 - ETA: 3s - loss: 0.6309 - accuracy: 0.62 - ETA: 3s - loss: 0.6302 - accuracy: 0.62 - ETA: 3s - loss: 0.6294 - accuracy: 0.62 - ETA: 3s - loss: 0.6285 - accuracy: 0.62 - ETA: 3s - loss: 0.6278 - accuracy: 0.62 - ETA: 2s - loss: 0.6273 - accuracy: 0.62 - ETA: 2s - loss: 0.6266 - accuracy: 0.62 - ETA: 2s - loss: 0.6256 - accuracy: 0.62 - ETA: 2s - loss: 0.6255 - accuracy: 0.62 - ETA: 2s - loss: 0.6258 - accuracy: 0.62 - ETA: 2s - loss: 0.6264 - accuracy: 0.62 - ETA: 2s - loss: 0.6255 - accuracy: 0.62 - ETA: 2s - loss: 0.6250 - accuracy: 0.62 - ETA: 2s - loss: 0.6247 - accuracy: 0.62 - ETA: 2s - loss: 0.6239 - accuracy: 0.62 - ETA: 2s - loss: 0.6229 - accuracy: 0.62 - ETA: 2s - loss: 0.6236 - accuracy: 0.62 - ETA: 2s - loss: 0.6232 - accuracy: 0.62 - ETA: 1s - loss: 0.6223 - accuracy: 0.63 - ETA: 1s - loss: 0.6214 - accuracy: 0.63 - ETA: 1s - loss: 0.6209 - accuracy: 0.63 - ETA: 1s - loss: 0.6198 - accuracy: 0.63 - ETA: 1s - loss: 0.6197 - accuracy: 0.63 - ETA: 1s - loss: 0.6193 - accuracy: 0.63 - ETA: 1s - loss: 0.6188 - accuracy: 0.63 - ETA: 1s - loss: 0.6186 - accuracy: 0.63 - ETA: 1s - loss: 0.6174 - accuracy: 0.63 - ETA: 1s - loss: 0.6167 - accuracy: 0.63 - ETA: 1s - loss: 0.6157 - accuracy: 0.63 - ETA: 1s - loss: 0.6153 - accuracy: 0.63 - ETA: 0s - loss: 0.6149 - accuracy: 0.63 - ETA: 0s - loss: 0.6141 - accuracy: 0.63 - ETA: 0s - loss: 0.6134 - accuracy: 0.63 - ETA: 0s - loss: 0.6133 - accuracy: 0.63 - ETA: 0s - loss: 0.6133 - accuracy: 0.63 - ETA: 0s - loss: 0.6134 - accuracy: 0.63 - ETA: 0s - loss: 0.6125 - accuracy: 0.63 - ETA: 0s - loss: 0.6121 - accuracy: 0.63 - ETA: 0s - loss: 0.6114 - accuracy: 0.63 - ETA: 0s - loss: 0.6113 - accuracy: 0.63 - ETA: 0s - loss: 0.6109 - accuracy: 0.64 - 15s 90ms/step - loss: 0.6099 - accuracy: 0.6411 - val_loss: 0.4979 - val_accuracy: 0.7444\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "169/169 [==============================] - ETA: 13s - loss: 0.4939 - accuracy: 0.750 - ETA: 11s - loss: 0.4938 - accuracy: 0.734 - ETA: 11s - loss: 0.5343 - accuracy: 0.729 - ETA: 11s - loss: 0.5234 - accuracy: 0.742 - ETA: 11s - loss: 0.5238 - accuracy: 0.731 - ETA: 11s - loss: 0.5180 - accuracy: 0.739 - ETA: 12s - loss: 0.5039 - accuracy: 0.754 - ETA: 12s - loss: 0.5108 - accuracy: 0.746 - ETA: 12s - loss: 0.5177 - accuracy: 0.736 - ETA: 11s - loss: 0.5105 - accuracy: 0.737 - ETA: 12s - loss: 0.5081 - accuracy: 0.738 - ETA: 12s - loss: 0.5223 - accuracy: 0.721 - ETA: 11s - loss: 0.5246 - accuracy: 0.716 - ETA: 11s - loss: 0.5218 - accuracy: 0.727 - ETA: 11s - loss: 0.5209 - accuracy: 0.729 - ETA: 11s - loss: 0.5184 - accuracy: 0.728 - ETA: 11s - loss: 0.5200 - accuracy: 0.727 - ETA: 10s - loss: 0.5163 - accuracy: 0.729 - ETA: 10s - loss: 0.5159 - accuracy: 0.727 - ETA: 10s - loss: 0.5182 - accuracy: 0.726 - ETA: 10s - loss: 0.5215 - accuracy: 0.724 - ETA: 10s - loss: 0.5225 - accuracy: 0.720 - ETA: 10s - loss: 0.5309 - accuracy: 0.718 - ETA: 10s - loss: 0.5264 - accuracy: 0.721 - ETA: 10s - loss: 0.5282 - accuracy: 0.716 - ETA: 10s - loss: 0.5301 - accuracy: 0.712 - ETA: 10s - loss: 0.5303 - accuracy: 0.711 - ETA: 10s - loss: 0.5278 - accuracy: 0.718 - ETA: 10s - loss: 0.5290 - accuracy: 0.715 - ETA: 9s - loss: 0.5259 - accuracy: 0.719 - ETA: 9s - loss: 0.5232 - accuracy: 0.72 - ETA: 9s - loss: 0.5189 - accuracy: 0.72 - ETA: 9s - loss: 0.5185 - accuracy: 0.72 - ETA: 9s - loss: 0.5210 - accuracy: 0.72 - ETA: 9s - loss: 0.5195 - accuracy: 0.72 - ETA: 9s - loss: 0.5184 - accuracy: 0.72 - ETA: 9s - loss: 0.5175 - accuracy: 0.72 - ETA: 9s - loss: 0.5152 - accuracy: 0.72 - ETA: 9s - loss: 0.5141 - accuracy: 0.72 - ETA: 9s - loss: 0.5150 - accuracy: 0.72 - ETA: 9s - loss: 0.5169 - accuracy: 0.72 - ETA: 9s - loss: 0.5146 - accuracy: 0.72 - ETA: 9s - loss: 0.5140 - accuracy: 0.72 - ETA: 9s - loss: 0.5124 - accuracy: 0.73 - ETA: 9s - loss: 0.5130 - accuracy: 0.72 - ETA: 9s - loss: 0.5126 - accuracy: 0.72 - ETA: 9s - loss: 0.5119 - accuracy: 0.73 - ETA: 8s - loss: 0.5098 - accuracy: 0.73 - ETA: 8s - loss: 0.5063 - accuracy: 0.73 - ETA: 8s - loss: 0.5066 - accuracy: 0.73 - ETA: 8s - loss: 0.5026 - accuracy: 0.74 - ETA: 8s - loss: 0.5025 - accuracy: 0.74 - ETA: 8s - loss: 0.5029 - accuracy: 0.74 - ETA: 8s - loss: 0.5001 - accuracy: 0.74 - ETA: 8s - loss: 0.5006 - accuracy: 0.74 - ETA: 8s - loss: 0.5010 - accuracy: 0.74 - ETA: 8s - loss: 0.5010 - accuracy: 0.74 - ETA: 8s - loss: 0.5027 - accuracy: 0.74 - ETA: 7s - loss: 0.5013 - accuracy: 0.74 - ETA: 7s - loss: 0.5045 - accuracy: 0.73 - ETA: 7s - loss: 0.5040 - accuracy: 0.73 - ETA: 7s - loss: 0.5040 - accuracy: 0.73 - ETA: 7s - loss: 0.5038 - accuracy: 0.73 - ETA: 7s - loss: 0.5028 - accuracy: 0.74 - ETA: 7s - loss: 0.5025 - accuracy: 0.74 - ETA: 7s - loss: 0.5018 - accuracy: 0.74 - ETA: 7s - loss: 0.5021 - accuracy: 0.74 - ETA: 7s - loss: 0.5036 - accuracy: 0.73 - ETA: 7s - loss: 0.5038 - accuracy: 0.73 - ETA: 7s - loss: 0.5025 - accuracy: 0.74 - ETA: 6s - loss: 0.5033 - accuracy: 0.73 - ETA: 6s - loss: 0.5025 - accuracy: 0.73 - ETA: 6s - loss: 0.5026 - accuracy: 0.73 - ETA: 6s - loss: 0.5021 - accuracy: 0.74 - ETA: 6s - loss: 0.5016 - accuracy: 0.74 - ETA: 6s - loss: 0.5023 - accuracy: 0.73 - ETA: 6s - loss: 0.5030 - accuracy: 0.73 - ETA: 6s - loss: 0.5028 - accuracy: 0.73 - ETA: 6s - loss: 0.5034 - accuracy: 0.73 - ETA: 6s - loss: 0.5033 - accuracy: 0.73 - ETA: 6s - loss: 0.5040 - accuracy: 0.73 - ETA: 6s - loss: 0.5044 - accuracy: 0.73 - ETA: 5s - loss: 0.5053 - accuracy: 0.73 - ETA: 5s - loss: 0.5049 - accuracy: 0.73 - ETA: 5s - loss: 0.5042 - accuracy: 0.73 - ETA: 5s - loss: 0.5031 - accuracy: 0.73 - ETA: 5s - loss: 0.5032 - accuracy: 0.73 - ETA: 5s - loss: 0.5042 - accuracy: 0.73 - ETA: 5s - loss: 0.5063 - accuracy: 0.73 - ETA: 5s - loss: 0.5063 - accuracy: 0.73 - ETA: 5s - loss: 0.5062 - accuracy: 0.73 - ETA: 5s - loss: 0.5063 - accuracy: 0.73 - ETA: 5s - loss: 0.5059 - accuracy: 0.73 - ETA: 5s - loss: 0.5066 - accuracy: 0.73 - ETA: 5s - loss: 0.5061 - accuracy: 0.73 - ETA: 5s - loss: 0.5050 - accuracy: 0.73 - ETA: 4s - loss: 0.5051 - accuracy: 0.73 - ETA: 4s - loss: 0.5044 - accuracy: 0.73 - ETA: 4s - loss: 0.5040 - accuracy: 0.73 - ETA: 4s - loss: 0.5044 - accuracy: 0.73 - ETA: 4s - loss: 0.5038 - accuracy: 0.73 - ETA: 4s - loss: 0.5043 - accuracy: 0.73 - ETA: 4s - loss: 0.5034 - accuracy: 0.73 - ETA: 4s - loss: 0.5033 - accuracy: 0.73 - ETA: 4s - loss: 0.5026 - accuracy: 0.73 - ETA: 4s - loss: 0.5032 - accuracy: 0.73 - ETA: 4s - loss: 0.5029 - accuracy: 0.73 - ETA: 4s - loss: 0.5024 - accuracy: 0.73 - ETA: 4s - loss: 0.5027 - accuracy: 0.73 - ETA: 4s - loss: 0.5024 - accuracy: 0.73 - ETA: 3s - loss: 0.5028 - accuracy: 0.73 - ETA: 3s - loss: 0.5016 - accuracy: 0.73 - ETA: 3s - loss: 0.5013 - accuracy: 0.73 - ETA: 3s - loss: 0.5011 - accuracy: 0.73 - ETA: 3s - loss: 0.5010 - accuracy: 0.73 - ETA: 3s - loss: 0.5011 - accuracy: 0.73 - ETA: 3s - loss: 0.5008 - accuracy: 0.73 - ETA: 3s - loss: 0.5007 - accuracy: 0.73 - ETA: 3s - loss: 0.5000 - accuracy: 0.73 - ETA: 3s - loss: 0.5017 - accuracy: 0.73 - ETA: 3s - loss: 0.5012 - accuracy: 0.73 - ETA: 3s - loss: 0.5009 - accuracy: 0.73 - ETA: 3s - loss: 0.5008 - accuracy: 0.73 - ETA: 3s - loss: 0.5024 - accuracy: 0.73 - ETA: 3s - loss: 0.5024 - accuracy: 0.73 - ETA: 2s - loss: 0.5023 - accuracy: 0.73 - ETA: 2s - loss: 0.5028 - accuracy: 0.73 - ETA: 2s - loss: 0.5036 - accuracy: 0.73 - ETA: 2s - loss: 0.5036 - accuracy: 0.73 - ETA: 2s - loss: 0.5033 - accuracy: 0.73 - ETA: 2s - loss: 0.5029 - accuracy: 0.73 - ETA: 2s - loss: 0.5029 - accuracy: 0.73 - ETA: 2s - loss: 0.5032 - accuracy: 0.73 - ETA: 2s - loss: 0.5028 - accuracy: 0.73 - ETA: 2s - loss: 0.5026 - accuracy: 0.73 - ETA: 2s - loss: 0.5032 - accuracy: 0.73 - ETA: 2s - loss: 0.5033 - accuracy: 0.73 - ETA: 2s - loss: 0.5037 - accuracy: 0.73 - ETA: 2s - loss: 0.5034 - accuracy: 0.73 - ETA: 1s - loss: 0.5032 - accuracy: 0.73 - ETA: 1s - loss: 0.5035 - accuracy: 0.73 - ETA: 1s - loss: 0.5031 - accuracy: 0.73 - ETA: 1s - loss: 0.5028 - accuracy: 0.73 - ETA: 1s - loss: 0.5038 - accuracy: 0.73 - ETA: 1s - loss: 0.5033 - accuracy: 0.73 - ETA: 1s - loss: 0.5028 - accuracy: 0.73 - ETA: 1s - loss: 0.5019 - accuracy: 0.73 - ETA: 1s - loss: 0.5018 - accuracy: 0.73 - ETA: 1s - loss: 0.5010 - accuracy: 0.73 - ETA: 1s - loss: 0.5015 - accuracy: 0.73 - ETA: 1s - loss: 0.5016 - accuracy: 0.73 - ETA: 1s - loss: 0.5020 - accuracy: 0.73 - ETA: 1s - loss: 0.5023 - accuracy: 0.73 - ETA: 1s - loss: 0.5017 - accuracy: 0.73 - ETA: 0s - loss: 0.5012 - accuracy: 0.73 - ETA: 0s - loss: 0.5005 - accuracy: 0.73 - ETA: 0s - loss: 0.5008 - accuracy: 0.73 - ETA: 0s - loss: 0.5008 - accuracy: 0.73 - ETA: 0s - loss: 0.5004 - accuracy: 0.73 - ETA: 0s - loss: 0.5000 - accuracy: 0.73 - ETA: 0s - loss: 0.5002 - accuracy: 0.73 - ETA: 0s - loss: 0.5009 - accuracy: 0.73 - ETA: 0s - loss: 0.5007 - accuracy: 0.73 - ETA: 0s - loss: 0.5000 - accuracy: 0.73 - ETA: 0s - loss: 0.4997 - accuracy: 0.73 - ETA: 0s - loss: 0.4992 - accuracy: 0.73 - ETA: 0s - loss: 0.4995 - accuracy: 0.73 - ETA: 0s - loss: 0.4995 - accuracy: 0.73 - 12s 72ms/step - loss: 0.4984 - accuracy: 0.7381 - val_loss: 0.4707 - val_accuracy: 0.7583\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "169/169 [==============================] - ETA: 13s - loss: 0.4609 - accuracy: 0.781 - ETA: 12s - loss: 0.4646 - accuracy: 0.765 - ETA: 14s - loss: 0.4832 - accuracy: 0.760 - ETA: 14s - loss: 0.4871 - accuracy: 0.765 - ETA: 13s - loss: 0.4953 - accuracy: 0.750 - ETA: 13s - loss: 0.4854 - accuracy: 0.760 - ETA: 12s - loss: 0.4795 - accuracy: 0.767 - ETA: 12s - loss: 0.4865 - accuracy: 0.761 - ETA: 11s - loss: 0.4962 - accuracy: 0.750 - ETA: 11s - loss: 0.4983 - accuracy: 0.743 - ETA: 11s - loss: 0.4967 - accuracy: 0.744 - ETA: 11s - loss: 0.5170 - accuracy: 0.724 - ETA: 11s - loss: 0.5160 - accuracy: 0.721 - ETA: 11s - loss: 0.5118 - accuracy: 0.729 - ETA: 11s - loss: 0.5096 - accuracy: 0.735 - ETA: 11s - loss: 0.5064 - accuracy: 0.736 - ETA: 11s - loss: 0.5108 - accuracy: 0.733 - ETA: 11s - loss: 0.5070 - accuracy: 0.734 - ETA: 11s - loss: 0.5083 - accuracy: 0.730 - ETA: 11s - loss: 0.5068 - accuracy: 0.732 - ETA: 11s - loss: 0.5120 - accuracy: 0.729 - ETA: 10s - loss: 0.5160 - accuracy: 0.721 - ETA: 10s - loss: 0.5199 - accuracy: 0.720 - ETA: 10s - loss: 0.5166 - accuracy: 0.720 - ETA: 10s - loss: 0.5202 - accuracy: 0.713 - ETA: 10s - loss: 0.5221 - accuracy: 0.716 - ETA: 10s - loss: 0.5229 - accuracy: 0.716 - ETA: 10s - loss: 0.5222 - accuracy: 0.723 - ETA: 10s - loss: 0.5225 - accuracy: 0.720 - ETA: 9s - loss: 0.5212 - accuracy: 0.724 - ETA: 9s - loss: 0.5185 - accuracy: 0.72 - ETA: 9s - loss: 0.5138 - accuracy: 0.72 - ETA: 9s - loss: 0.5128 - accuracy: 0.73 - ETA: 9s - loss: 0.5166 - accuracy: 0.72 - ETA: 9s - loss: 0.5151 - accuracy: 0.72 - ETA: 9s - loss: 0.5147 - accuracy: 0.72 - ETA: 9s - loss: 0.5129 - accuracy: 0.73 - ETA: 9s - loss: 0.5107 - accuracy: 0.73 - ETA: 9s - loss: 0.5094 - accuracy: 0.73 - ETA: 9s - loss: 0.5115 - accuracy: 0.73 - ETA: 8s - loss: 0.5129 - accuracy: 0.73 - ETA: 8s - loss: 0.5111 - accuracy: 0.73 - ETA: 8s - loss: 0.5109 - accuracy: 0.73 - ETA: 8s - loss: 0.5093 - accuracy: 0.73 - ETA: 8s - loss: 0.5100 - accuracy: 0.73 - ETA: 8s - loss: 0.5093 - accuracy: 0.73 - ETA: 8s - loss: 0.5078 - accuracy: 0.73 - ETA: 8s - loss: 0.5052 - accuracy: 0.73 - ETA: 8s - loss: 0.5026 - accuracy: 0.74 - ETA: 8s - loss: 0.5026 - accuracy: 0.74 - ETA: 8s - loss: 0.5018 - accuracy: 0.74 - ETA: 8s - loss: 0.5015 - accuracy: 0.74 - ETA: 8s - loss: 0.5021 - accuracy: 0.74 - ETA: 7s - loss: 0.4999 - accuracy: 0.74 - ETA: 7s - loss: 0.5003 - accuracy: 0.74 - ETA: 7s - loss: 0.5007 - accuracy: 0.74 - ETA: 7s - loss: 0.5002 - accuracy: 0.74 - ETA: 7s - loss: 0.5019 - accuracy: 0.74 - ETA: 7s - loss: 0.5012 - accuracy: 0.74 - ETA: 7s - loss: 0.5042 - accuracy: 0.74 - ETA: 7s - loss: 0.5043 - accuracy: 0.74 - ETA: 7s - loss: 0.5061 - accuracy: 0.73 - ETA: 7s - loss: 0.5060 - accuracy: 0.73 - ETA: 7s - loss: 0.5061 - accuracy: 0.73 - ETA: 7s - loss: 0.5058 - accuracy: 0.73 - ETA: 7s - loss: 0.5054 - accuracy: 0.73 - ETA: 7s - loss: 0.5065 - accuracy: 0.73 - ETA: 6s - loss: 0.5086 - accuracy: 0.73 - ETA: 6s - loss: 0.5091 - accuracy: 0.73 - ETA: 6s - loss: 0.5084 - accuracy: 0.73 - ETA: 6s - loss: 0.5095 - accuracy: 0.73 - ETA: 6s - loss: 0.5091 - accuracy: 0.73 - ETA: 6s - loss: 0.5093 - accuracy: 0.73 - ETA: 6s - loss: 0.5089 - accuracy: 0.73 - ETA: 6s - loss: 0.5088 - accuracy: 0.73 - ETA: 6s - loss: 0.5099 - accuracy: 0.73 - ETA: 6s - loss: 0.5110 - accuracy: 0.73 - ETA: 6s - loss: 0.5105 - accuracy: 0.73 - ETA: 6s - loss: 0.5109 - accuracy: 0.73 - ETA: 6s - loss: 0.5103 - accuracy: 0.73 - ETA: 6s - loss: 0.5109 - accuracy: 0.73 - ETA: 6s - loss: 0.5112 - accuracy: 0.73 - ETA: 6s - loss: 0.5117 - accuracy: 0.73 - ETA: 6s - loss: 0.5112 - accuracy: 0.72 - ETA: 5s - loss: 0.5103 - accuracy: 0.73 - ETA: 5s - loss: 0.5090 - accuracy: 0.73 - ETA: 5s - loss: 0.5088 - accuracy: 0.73 - ETA: 5s - loss: 0.5097 - accuracy: 0.73 - ETA: 5s - loss: 0.5113 - accuracy: 0.72 - ETA: 5s - loss: 0.5113 - accuracy: 0.73 - ETA: 5s - loss: 0.5107 - accuracy: 0.72 - ETA: 5s - loss: 0.5109 - accuracy: 0.72 - ETA: 5s - loss: 0.5104 - accuracy: 0.72 - ETA: 5s - loss: 0.5110 - accuracy: 0.72 - ETA: 5s - loss: 0.5104 - accuracy: 0.72 - ETA: 5s - loss: 0.5095 - accuracy: 0.73 - ETA: 5s - loss: 0.5095 - accuracy: 0.73 - ETA: 5s - loss: 0.5087 - accuracy: 0.73 - ETA: 4s - loss: 0.5082 - accuracy: 0.73 - ETA: 4s - loss: 0.5087 - accuracy: 0.73 - ETA: 4s - loss: 0.5084 - accuracy: 0.73 - ETA: 4s - loss: 0.5086 - accuracy: 0.73 - ETA: 4s - loss: 0.5079 - accuracy: 0.73 - ETA: 4s - loss: 0.5078 - accuracy: 0.73 - ETA: 4s - loss: 0.5072 - accuracy: 0.73 - ETA: 4s - loss: 0.5075 - accuracy: 0.73 - ETA: 4s - loss: 0.5076 - accuracy: 0.72 - ETA: 4s - loss: 0.5073 - accuracy: 0.72 - ETA: 4s - loss: 0.5075 - accuracy: 0.72 - ETA: 4s - loss: 0.5074 - accuracy: 0.72 - ETA: 4s - loss: 0.5081 - accuracy: 0.72 - ETA: 4s - loss: 0.5077 - accuracy: 0.72 - ETA: 3s - loss: 0.5076 - accuracy: 0.72 - ETA: 3s - loss: 0.5078 - accuracy: 0.73 - ETA: 3s - loss: 0.5078 - accuracy: 0.72 - ETA: 3s - loss: 0.5081 - accuracy: 0.72 - ETA: 3s - loss: 0.5078 - accuracy: 0.73 - ETA: 3s - loss: 0.5079 - accuracy: 0.73 - ETA: 3s - loss: 0.5074 - accuracy: 0.73 - ETA: 3s - loss: 0.5089 - accuracy: 0.72 - ETA: 3s - loss: 0.5086 - accuracy: 0.72 - ETA: 3s - loss: 0.5084 - accuracy: 0.72 - ETA: 3s - loss: 0.5082 - accuracy: 0.72 - ETA: 3s - loss: 0.5092 - accuracy: 0.72 - ETA: 3s - loss: 0.5093 - accuracy: 0.72 - ETA: 3s - loss: 0.5091 - accuracy: 0.72 - ETA: 3s - loss: 0.5095 - accuracy: 0.72 - ETA: 2s - loss: 0.5101 - accuracy: 0.72 - ETA: 2s - loss: 0.5101 - accuracy: 0.72 - ETA: 2s - loss: 0.5097 - accuracy: 0.72 - ETA: 2s - loss: 0.5093 - accuracy: 0.72 - ETA: 2s - loss: 0.5093 - accuracy: 0.72 - ETA: 2s - loss: 0.5097 - accuracy: 0.72 - ETA: 2s - loss: 0.5091 - accuracy: 0.72 - ETA: 2s - loss: 0.5088 - accuracy: 0.72 - ETA: 2s - loss: 0.5092 - accuracy: 0.72 - ETA: 2s - loss: 0.5092 - accuracy: 0.72 - ETA: 2s - loss: 0.5097 - accuracy: 0.72 - ETA: 2s - loss: 0.5092 - accuracy: 0.72 - ETA: 2s - loss: 0.5089 - accuracy: 0.72 - ETA: 2s - loss: 0.5090 - accuracy: 0.72 - ETA: 1s - loss: 0.5083 - accuracy: 0.73 - ETA: 1s - loss: 0.5080 - accuracy: 0.73 - ETA: 1s - loss: 0.5094 - accuracy: 0.72 - ETA: 1s - loss: 0.5089 - accuracy: 0.72 - ETA: 1s - loss: 0.5082 - accuracy: 0.73 - ETA: 1s - loss: 0.5071 - accuracy: 0.73 - ETA: 1s - loss: 0.5070 - accuracy: 0.73 - ETA: 1s - loss: 0.5062 - accuracy: 0.73 - ETA: 1s - loss: 0.5067 - accuracy: 0.73 - ETA: 1s - loss: 0.5068 - accuracy: 0.73 - ETA: 1s - loss: 0.5071 - accuracy: 0.73 - ETA: 1s - loss: 0.5074 - accuracy: 0.73 - ETA: 1s - loss: 0.5067 - accuracy: 0.73 - ETA: 1s - loss: 0.5062 - accuracy: 0.73 - ETA: 0s - loss: 0.5056 - accuracy: 0.73 - ETA: 0s - loss: 0.5058 - accuracy: 0.73 - ETA: 0s - loss: 0.5059 - accuracy: 0.73 - ETA: 0s - loss: 0.5054 - accuracy: 0.73 - ETA: 0s - loss: 0.5051 - accuracy: 0.73 - ETA: 0s - loss: 0.5050 - accuracy: 0.73 - ETA: 0s - loss: 0.5056 - accuracy: 0.73 - ETA: 0s - loss: 0.5054 - accuracy: 0.73 - ETA: 0s - loss: 0.5045 - accuracy: 0.73 - ETA: 0s - loss: 0.5043 - accuracy: 0.73 - ETA: 0s - loss: 0.5037 - accuracy: 0.73 - ETA: 0s - loss: 0.5039 - accuracy: 0.73 - ETA: 0s - loss: 0.5041 - accuracy: 0.73 - 13s 78ms/step - loss: 0.5031 - accuracy: 0.7330 - val_loss: 0.4715 - val_accuracy: 0.7537\n"
     ]
    }
   ],
   "source": [
    "num_features = len(r._reber_letters) + 1 # include padding token\n",
    "# TODO: how to mask in the embedding\n",
    "input_node = ak.Input()\n",
    "output_node = ak.Embedding(max_features=num_features)(input_node)\n",
    "output_node = ak.RNNBlock(\n",
    "    bidirectional=False,\n",
    "    # num_layers=1,\n",
    "    layer_type='gru'\n",
    ")(output_node)\n",
    "output_node = ak.ClassificationHead()(output_node)\n",
    "clf = ak.AutoModel(\n",
    "    inputs=input_node,\n",
    "    outputs=output_node,\n",
    "    max_trials=4\n",
    ")\n",
    "clf.fit(X_train.to_numpy(), y_train.to_numpy(), epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 80000 samples, validate on 20000 samples\n",
      "Epoch 1/2\n",
      "80000/80000 [==============================] - 17s 217us/step - loss: 0.6903 - accuracy: 0.5657 - val_loss: 0.6851 - val_accuracy: 0.4983\n",
      "Epoch 2/2\n",
      "80000/80000 [==============================] - 18s 224us/step - loss: 0.6665 - accuracy: 0.5379 - val_loss: 0.6395 - val_accuracy: 0.6458\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    validation_data=(X_test, y_test),\n",
    "    epochs=1,\n",
    "    batch_size=5000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-2.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-2.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-2.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-2.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-2.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-2.bias\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.6689042 ],\n",
       "       [0.50881684],\n",
       "       [0.00367002],\n",
       "       [0.0018216 ],\n",
       "       [0.00168793],\n",
       "       [0.00123131]], dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "strs = [\n",
    "    \"BPBTSSXXVVEPE\", # true reber\n",
    "    \"BPBTSSSSXXVPXTTTVPSETE\", # second and second to last not the same, but reber inbetween\n",
    "    \"BPBPTTTTTVPBTTTTVPSEPT\", # second and second to last the same, but not reber in middle\n",
    "    \"BBBBBBBBBBB\",\n",
    "    \"XXXXXXXXXXXXXX\",\n",
    "    \"TTTTTTTTTTTTTTTTTTT\"\n",
    "]\n",
    "x = np.array([\n",
    "        r.encode_as_padded_ints(s) for s in strs\n",
    "    ])\n",
    "dataset = clf._process_x(x, False)\n",
    "dataset = dataset.batch(6)\n",
    "model = clf.tuner.get_best_model()\n",
    "model.predict(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly, XXXXXXXXXX is not in the reber grammar, and yet the model predicted with 98.6% certainty that it was. This means that I need to make my examples of non-reber much more random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.76853848"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "119879    0\n",
       "103694    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[-5:-3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
